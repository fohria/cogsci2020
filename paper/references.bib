@ARTICLE{Grillner2016-wp,
  title     = "The Basal Ganglia Over 500 Million Years",
  author    = "Grillner, Sten and Robertson, Brita",
  abstract  = "The lamprey belongs to the phylogenetically oldest group of
               vertebrates that diverged from the mammalian evolutionary line
               560 million years ago. A comparison between the lamprey and
               mammalian basal ganglia establishes a detailed similarity
               regarding its input from cortex/pallium and thalamus, as well as
               its intrinsic organisation and projections of the output nuclei.
               This means that the basal ganglia circuits now present in
               rodents and primates most likely had evolved already at the dawn
               of vertebrate evolution. This includes the 'direct pathway' with
               striatal projection neurons (SPNs) expressing dopamine D1
               receptors, which act to inhibit the tonically active GABAergic
               output neurons in globus pallidus interna and substantia nigra
               pars reticulata that at rest keep the brainstem motor centres
               under tonic inhibition. The 'indirect pathway' with dopamine D2
               receptor-expressing SPNs and intrinsic basal ganglia nuclei is
               also conserved. The net effect of the direct pathway is to
               disinhibit brainstem motor centres and release motor programs,
               while the indirect pathway instead will suppress motor activity.
               Transmitters, connectivity and membrane properties are virtually
               identical in lamprey and rodent basal ganglia. We predict that
               the basal ganglia contains a series of modules each controlling
               a given pattern of behaviour including locomotion,
               eye-movements, posture, and chewing that contain both the direct
               pathway to release a motor program and the indirect pathway to
               inhibit competing behaviours. The phasic dopamine input serves
               value-based decisions and motor learning. During vertebrate
               evolution with a progressively more diverse motor behaviour, the
               number of modules will have increased progressively. These new
               modules with a similar design will be used to control newly
               developed patterns of behaviour - a process referred to as
               exaptation.",
  journal   = "Curr. Biol.",
  publisher = "Elsevier",
  volume    =  26,
  number    =  20,
  pages     = "R1088--R1100",
  month     =  oct,
  year      =  2016,
  language  = "en"
}

@BOOK{Sutton2018-an,
  title     = "Reinforcement Learning: An Introduction 2nd edition",
  author    = "Sutton, Richard S and Barto, Andrew G",
  editor    = "Bach, Francis",
  publisher = "MIT Press, Cambridge, MA",
  series    = "Adaptive Computation and Machine Learning",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Stalnaker2015-ix,
  title    = "What the orbitofrontal cortex does not do",
  author   = "Stalnaker, Thomas A and Cooch, Nisha K and Schoenbaum, Geoffrey",
  abstract = "The number of papers about the orbitofrontal cortex (OFC) has
              grown from 1 per month in 1987 to a current rate of over 50 per
              month. This publication stream has implicated the OFC in nearly
              every function known to cognitive neuroscience and in most
              neuropsychiatric diseases. However, new ideas about OFC function
              are typically based on limited data sets and often ignore or
              minimize competing ideas or contradictory findings. Yet true
              progress in our understanding of an area's function comes as much
              from invalidating existing ideas as proposing new ones. Here we
              consider the proposed roles for OFC, critically examining the
              level of support for these claims and highlighting the data that
              call them into question.",
  journal  = "Nat. Neurosci.",
  volume   =  18,
  number   =  5,
  pages    = "620--627",
  month    =  may,
  year     =  2015,
  language = "en"
}

@ARTICLE{Takahashi2008-ue,
  title    = "Silencing the critics: understanding the effects of cocaine
              sensitization on dorsolateral and ventral striatum in the context
              of an actor/critic model",
  author   = "Takahashi, Yuji and Schoenbaum, Geoffrey and Niv, Yael",
  abstract = "A critical problem in daily decision making is how to choose
              actions now in order to bring about rewards later. Indeed, many
              of our actions have long-term consequences, and it is important
              to not be myopic in balancing the pros and cons of different
              options, but rather to take into account both immediate and
              delayed consequences of actions. Failures to do so may be
              manifest as persistent, maladaptive decision-making, one example
              of which is addiction where behavior seems to be driven by the
              immediate positive experiences with drugs, despite the delayed
              adverse consequences. A recent study by Takahashi et al. (2007)
              investigated the effects of cocaine sensitization on decision
              making in rats and showed that drug use resulted in altered
              representations in the ventral striatum and the dorsolateral
              striatum, areas that have been implicated in the neural
              instantiation of a computational solution to optimal long-term
              actions selection called the Actor/Critic framework. In this
              Focus article we discuss their results and offer a computational
              interpretation in terms of drug-induced impairments in the
              Critic. We first survey the different lines of evidence linking
              the subparts of the striatum to the Actor/Critic framework, and
              then suggest two possible scenarios of breakdown that are
              suggested by Takahashi et al.'s (2007) data. As both are
              compatible with the current data, we discuss their different
              predictions and how these could be empirically tested in order to
              further elucidate (and hopefully inch towards curing) the neural
              basis of drug addiction.",
  journal  = "Front. Neurosci.",
  volume   =  2,
  number   =  1,
  pages    = "86--99",
  month    =  jul,
  year     =  2008,
  keywords = "Actor/Critic; cocaine; reinforcement learning; striatum",
  language = "en"
}

@ARTICLE{Matsumoto2007-fs,
  title     = "Lateral habenula as a source of negative reward signals in
               dopamine neurons",
  author    = "Matsumoto, Masayuki and Hikosaka, Okihide",
  abstract  = "Midbrain dopamine neurons are key components of the brain's
               reward system, which is thought to guide reward-seeking
               behaviours. Although recent studies have shown how dopamine
               neurons respond to rewards and sensory stimuli predicting
               reward, it is unclear which parts of the brain provide dopamine
               neurons with signals necessary for these actions. Here we show
               that the primate lateral habenula, part of the structure called
               the epithalamus, is a major candidate for a source of negative
               reward-related signals in dopamine neurons. We recorded the
               activity of habenula neurons and dopamine neurons while rhesus
               monkeys were performing a visually guided saccade task with
               positionally biased reward outcomes. Many habenula neurons were
               excited by a no-reward-predicting target and inhibited by a
               reward-predicting target. In contrast, dopamine neurons were
               excited and inhibited by reward-predicting and
               no-reward-predicting targets, respectively. Each time the
               rewarded and unrewarded positions were reversed, both habenula
               and dopamine neurons reversed their responses as the bias in
               saccade latency reversed. In unrewarded trials, the excitation
               of habenula neurons started earlier than the inhibition of
               dopamine neurons. Furthermore, weak electrical stimulation of
               the lateral habenula elicited strong inhibitions in dopamine
               neurons. These results suggest that the inhibitory input from
               the lateral habenula plays an important role in determining the
               reward-related activity of dopamine neurons.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  447,
  number    =  7148,
  pages     = "1111--1115",
  month     =  jun,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Daw2005-ti,
  title     = "Uncertainty-based competition between prefrontal and
               dorsolateral striatal systems for behavioral control",
  author    = "Daw, Nathaniel D and Niv, Yael and Dayan, Peter",
  abstract  = "A broad range of neural and behavioral data suggests that the
               brain contains multiple systems for behavioral choice, including
               one associated with prefrontal cortex and another with
               dorsolateral striatum. However, such a surfeit of control raises
               an additional choice problem: how to arbitrate between the
               systems when they disagree. Here, we consider dual-action choice
               systems from a normative perspective, using the computational
               theory of reinforcement learning. We identify a key trade-off
               pitting computational simplicity against the flexible and
               statistically efficient use of experience. The trade-off is
               realized in a competition between the dorsolateral striatal and
               prefrontal systems. We suggest a Bayesian principle of
               arbitration between them according to uncertainty, so each
               controller is deployed when it should be most accurate. This
               provides a unifying account of a wealth of experimental evidence
               about the factors favoring dominance by either system.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  8,
  number    =  12,
  pages     = "1704--1711",
  year      =  2005,
  language  = "en"
}

@ARTICLE{Corbit2000-rv,
  title     = "The role of the hippocampus in instrumental conditioning",
  author    = "Corbit, L H and Balleine, B W",
  abstract  = "Considerable evidence suggests that, in instrumental
               conditioning, rats can encode both the specific action-outcome
               associations to which they are exposed and the degree to which
               an action is causal in producing its associated outcome. Three
               experiments assessed the involvement of the hippocampus in
               encoding these aspects of instrumental learning. In each study,
               rats with electrolytic lesions of the dorsal hippocampus and
               sham-lesioned controls were trained while hungry to press two
               levers, each of which delivered a unique food outcome.
               Experiments 1A and 1B used an outcome devaluation procedure to
               assess the effects of the lesion on encoding the action-outcome
               relationship. After training, one of the two outcomes was
               devalued using a specific satiety procedure, after which
               performance on the two levers was assessed in a choice
               extinction test. The lesion had no detectable effect on either
               the acquisition of instrumental performance or on the rats'
               sensitivity to outcome devaluation; lesion and sham groups both
               reduced responding on the lever associated with the devalued
               outcome compared with the other lever. In experiment 2, the
               sensitivity of hippocampal rats to the causal efficacy of their
               actions was assessed by selectively degrading the contingency
               between one of the actions and its associated outcome. Whereas
               sham rats selectively reduced performance on the lever for which
               the action-outcome contingency had been degraded, hippocampal
               rats did not. These results suggest that, in instrumental
               conditioning, lesions of the dorsal hippocampus selectively
               impair the ability of rats to represent the causal relationship
               between an action and its consequences.",
  journal   = "J. Neurosci.",
  publisher = "Soc Neuroscience",
  volume    =  20,
  number    =  11,
  pages     = "4233--4239",
  month     =  jun,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Chan2016-eo,
  title    = "A Probability Distribution over Latent Causes, in the
              Orbitofrontal Cortex",
  author   = "Chan, Stephanie C Y and Niv, Yael and Norman, Kenneth A",
  abstract = "UNLABELLED: The orbitofrontal cortex (OFC) has been implicated in
              both the representation of ``state,'' in studies of reinforcement
              learning and decision making, and also in the representation of
              ``schemas,'' in studies of episodic memory. Both of these
              cognitive constructs require a similar inference about the
              underlying situation or ``latent cause'' that generates our
              observations at any given time. The statistically optimal
              solution to this inference problem is to use Bayes' rule to
              compute a posterior probability distribution over latent causes.
              To test whether such a posterior probability distribution is
              represented in the OFC, we tasked human participants with
              inferring a probability distribution over four possible latent
              causes, based on their observations. Using fMRI pattern
              similarity analyses, we found that BOLD activity in the OFC is
              best explained as representing the (log-transformed) posterior
              distribution over latent causes. Furthermore, this pattern
              explained OFC activity better than other task-relevant
              alternatives, such as the most probable latent cause, the most
              recent observation, or the uncertainty over latent causes.
              SIGNIFICANCE STATEMENT: Our world is governed by hidden (latent)
              causes that we cannot observe, but which generate the
              observations we see. A range of high-level cognitive processes
              require inference of a probability distribution (or ``belief
              distribution'') over the possible latent causes that might be
              generating our current observations. This is true for
              reinforcement learning and decision making (where the latent
              cause comprises the true ``state'' of the task), and for episodic
              memory (where memories are believed to be organized by the
              inferred situation or ``schema''). Using fMRI, we show that this
              belief distribution over latent causes is encoded in patterns of
              brain activity in the orbitofrontal cortex, an area that has been
              separately implicated in the representations of both states and
              schemas.",
  journal  = "J. Neurosci.",
  volume   =  36,
  number   =  30,
  pages    = "7817--7828",
  year     =  2016,
  keywords = "Bayes' rule; context; posterior distribution; schemas; state
              representation; ventromedial prefrontal cortex",
  language = "en"
}

@ARTICLE{Schacter2012-lk,
  title     = "The future of memory: remembering, imagining, and the brain",
  author    = "Schacter, Daniel L and Addis, Donna Rose and Hassabis, Demis and
               Martin, Victoria C and Spreng, R Nathan and Szpunar, Karl K",
  abstract  = "During the past few years, there has been a dramatic increase in
               research examining the role of memory in imagination and future
               thinking. This work has revealed striking similarities between
               remembering the past and imagining or simulating the future,
               including the finding that a common brain network underlies both
               memory and imagination. Here, we discuss a number of key points
               that have emerged during recent years, focusing in particular on
               the importance of distinguishing between temporal and
               nontemporal factors in analyses of memory and imagination, the
               nature of differences between remembering the past and imagining
               the future, the identification of component processes that
               comprise the default network supporting memory-based
               simulations, and the finding that this network can couple
               flexibly with other networks to support complex goal-directed
               simulations. This growing area of research has broadened our
               conception of memory by highlighting the many ways in which
               memory supports adaptive functioning.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  76,
  number    =  4,
  pages     = "677--694",
  year      =  2012,
  language  = "en"
}

@INCOLLECTION{Hoffman2009-cm,
  title     = "The Interface Theory of Perception",
  booktitle = "Object Categorization: Computer and Human Vision Perspectives",
  author    = "Hoffman, Donald D",
  editor    = "{Sven Dickinson, Michael Tarr, Ales Leonardis, Bernt Schiele}",
  publisher = "Oxford University Press",
  pages     = "148--265",
  year      =  2009
}

@ARTICLE{Ash2012-ci,
  title     = "Investigating Insight as Sudden Learning",
  author    = "Ash, Ivan K and Jee, Benjamin D and Wiley, Jennifer",
  abstract  = "Gestalt psychologists proposed two distinct learning mechanisms.
               Associative learning occurs gradually through the repeated
               co-occurrence of external stimuli or memories. Insight learning
               occurs suddenly when people discover new relationships within
               their prior knowledge as a result of reasoning or problem
               solving processes that re-organize or restructure that
               knowledge. While there has been a considerable amount of
               research on the type of problem solving processes described by
               the Gestalt psychologists, less has focused on the learning that
               results from these processes. This paper begins with a
               historical review of the Gestalt theory of insight learning.
               Next, the core assumptions of Gestalt insight learning theory
               are empirically tested with a study that investigated the
               relationships among problem difficulty, impasse, initial problem
               representations, and resolution effects. Finally, Gestalt
               insight learning theory is discussed in relation to modern
               information processing theories of comprehension and memory
               formation.",
  journal   = "The Journal of Problem Solving",
  publisher = "docs.lib.purdue.edu",
  volume    =  4,
  number    =  2,
  pages     = "2",
  year      =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Li2006-py,
  title     = "Towards a unified theory of state abstraction for {MDPs}",
  booktitle = "{ISAIM}",
  author    = "Li, Lihong and Walsh, Thomas J and Littman, Michael L",
  abstract  = "State abstraction (or state aggregation) has been extensively
               studied in the fields of artificial intelligence and operations
               research. Instead of working in the ground state space, the
               decision maker usually finds solutions in the abstract state
               space much faster by treating …",
  publisher = "rbr.cs.umass.edu",
  year      =  2006
}

@ARTICLE{Sutton1981-ze,
  title    = "Toward a modern theory of adaptive networks: expectation and
              prediction",
  author   = "Sutton, R S and Barto, A G",
  journal  = "Psychol. Rev.",
  volume   =  88,
  number   =  2,
  pages    = "135--170",
  year     =  1981,
  language = "en"
}

@ARTICLE{Daw2011-ky,
  title    = "Model-based influences on humans' choices and striatal prediction
              errors",
  author   = "Daw, Nathaniel D and Gershman, Samuel J and Seymour, Ben and
              Dayan, Peter and Dolan, Raymond J",
  abstract = "The mesostriatal dopamine system is prominently implicated in
              model-free reinforcement learning, with fMRI BOLD signals in
              ventral striatum notably covarying with model-free prediction
              errors. However, latent learning and devaluation studies show
              that behavior also shows hallmarks of model-based planning, and
              the interaction between model-based and model-free values,
              prediction errors, and preferences is underexplored. We designed
              a multistep decision task in which model-based and model-free
              influences on human choice behavior could be distinguished. By
              showing that choices reflected both influences we could then test
              the purity of the ventral striatal BOLD signal as a model-free
              report. Contrary to expectations, the signal reflected both
              model-free and model-based predictions in proportions matching
              those that best explained choice behavior. These results
              challenge the notion of a separate model-free learner and suggest
              a more integrated computational architecture for high-level human
              decision-making.",
  journal  = "Neuron",
  volume   =  69,
  number   =  6,
  pages    = "1204--1215",
  year     =  2011,
  language = "en"
}

@INPROCEEDINGS{Koenderink2011-ud,
  title      = "Vision as a user interface",
  booktitle  = "Human Vision and Electronic Imaging {XVI}",
  author     = "Koenderink, Jan",
  abstract   = "The egg-rolling behavior of the graylag goose is an often
                quoted example of a fixed-action pattern. The bird will even
                attempt to roll a brick back to its nest! Despite excellent
                visual acuity it apparently takes a brick for an egg.``
                Evolution optimizes utility, not veridicality. Yet textbooks
                take it for a fact that human vision evolved so as to approach
                veridical perception. How do humans manage to dodge the laws of
                evolution? I will show that they don't, but that human vision
                is an idiosyncratic user interface. By way of an example I
                consider the case of pictorial perception. Gleaning information
                from still images is an important human ability and is likely
                to remain so for the foreseeable future. I will discuss a
                number of instances of extreme non-veridicality and huge
                inter-observer variability. Despite their importance in
                applications (information dissemination, personnel
                selection,...) such huge effects have remained undocumented in
                the literature, although they can be traced to artistic
                conventions. The reason appears to be that conventional
                psychophysics-by design-fails to address the qualitative, that
                is the meaningful, aspects of visual awareness whereas this is
                the very target of the visual arts.",
  publisher  = "International Society for Optics and Photonics",
  volume     =  7865,
  pages      = "786504",
  year       =  2011,
  keywords   = "Vision; User interfaces; Ethology; Evolution of vision;
                Veridicality of perception;",
  conference = "Human Vision and Electronic Imaging XVI"
}

@ARTICLE{Friston2014-fb,
  title    = "The anatomy of choice: dopamine and decision-making",
  author   = "Friston, Karl and Schwartenbeck, Philipp and FitzGerald, Thomas
              and Moutoussis, Michael and Behrens, Timothy and Dolan, Raymond J",
  abstract = "This paper considers goal-directed decision-making in terms of
              embodied or active inference. We associate bounded rationality
              with approximate Bayesian inference that optimizes a free energy
              bound on model evidence. Several constructs such as expected
              utility, exploration or novelty bonuses, softmax choice rules and
              optimism bias emerge as natural consequences of free energy
              minimization. Previous accounts of active inference have focused
              on predictive coding. In this paper, we consider variational
              Bayes as a scheme that the brain might use for approximate
              Bayesian inference. This scheme provides formal constraints on
              the computational anatomy of inference and action, which appear
              to be remarkably consistent with neuroanatomy. Active inference
              contextualizes optimal decision theory within embodied inference,
              where goals become prior beliefs. For example, expected utility
              theory emerges as a special case of free energy minimization,
              where the sensitivity or inverse temperature (associated with
              softmax functions and quantal response equilibria) has a unique
              and Bayes-optimal solution. Crucially, this sensitivity
              corresponds to the precision of beliefs about behaviour. The
              changes in precision during variational updates are remarkably
              reminiscent of empirical dopaminergic responses-and they may
              provide a new perspective on the role of dopamine in assimilating
              reward prediction errors to optimize decision-making.",
  journal  = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  volume   =  369,
  number   =  1655,
  year     =  2014,
  keywords = "Bayesian inference; active inference; agency; bounded
              rationality; free energy; utility theory",
  language = "en"
}

@ARTICLE{Morris2006-xp,
  title     = "Midbrain dopamine neurons encode decisions for future action",
  author    = "Morris, Genela and Nevet, Alon and Arkadir, David and Vaadia,
               Eilon and Bergman, Hagai",
  abstract  = "Current models of the basal ganglia and dopamine neurons
               emphasize their role in reinforcement learning. However, the
               role of dopamine neurons in decision making is still unclear. We
               recorded from dopamine neurons in monkeys engaged in two types
               of trial: reference trials in an instructed-choice task and
               decision trials in a two-armed bandit decision task. We show
               that the activity of dopamine neurons in the decision setting is
               modulated according to the value of the upcoming action.
               Moreover, analysis of the probability matching strategy in the
               decision trials revealed that the dopamine population activity
               and not the reward during reference trials determines choice
               behavior. Because dopamine neurons do not have spatial or motor
               properties, we conclude that immediate decisions are likely to
               be generated elsewhere and conveyed to the dopamine neurons,
               which play a role in shaping long-term decision policy through
               dynamic modulation of the efficacy of basal ganglia synapses.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  9,
  number    =  8,
  pages     = "1057--1063",
  year      =  2006,
  language  = "en"
}

@ARTICLE{Schultz2015-oz,
  title     = "Neuronal Reward and Decision Signals: From Theories to Data",
  author    = "Schultz, Wolfram",
  abstract  = "Rewards are crucial objects that induce learning, approach
               behavior, choices, and emotions. Whereas emotions are difficult
               to investigate in animals, the learning function is mediated by
               neuronal reward prediction error signals which implement basic
               constructs of reinforcement learning theory. These signals are
               found in dopamine neurons, which emit a global reward signal to
               striatum and frontal cortex, and in specific neurons in
               striatum, amygdala, and frontal cortex projecting to select
               neuronal populations. The approach and choice functions involve
               subjective value, which is objectively assessed by behavioral
               choices eliciting internal, subjective reward preferences.
               Utility is the formal mathematical characterization of
               subjective value and a prime decision variable in economic
               choice theory. It is coded as utility prediction error by phasic
               dopamine responses. Utility can incorporate various influences,
               including risk, delay, effort, and social interaction.
               Appropriate for formal decision mechanisms, rewards are coded as
               object value, action value, difference value, and chosen value
               by specific neurons. Although all reward, reinforcement, and
               decision variables are theoretical constructs, their neuronal
               signals constitute measurable physical implementations and as
               such confirm the validity of these concepts. The neuronal reward
               signals provide guidance for behavior while constraining the
               free will to act.",
  journal   = "Physiol. Rev.",
  publisher = "Am Physiological Soc",
  volume    =  95,
  number    =  3,
  pages     = "853--951",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Schultz2016-ul,
  title    = "Dopamine reward prediction-error signalling: a two-component
              response",
  author   = "Schultz, Wolfram",
  abstract = "Environmental stimuli and objects, including rewards, are often
              processed sequentially in the brain. Recent work suggests that
              the phasic dopamine reward prediction-error response follows a
              similar sequential pattern. An initial brief, unselective and
              highly sensitive increase in activity unspecifically detects a
              wide range of environmental stimuli, then quickly evolves into
              the main response component, which reflects subjective reward
              value and utility. This temporal evolution allows the dopamine
              reward prediction-error signal to optimally combine speed and
              accuracy.",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  17,
  number   =  3,
  pages    = "183--195",
  year     =  2016,
  language = "en"
}

@ARTICLE{Wagner2017-xl,
  title    = "Cerebellar granule cells encode the expectation of reward",
  author   = "Wagner, Mark J and Kim, Tony Hyun and Savall, Joan and Schnitzer,
              Mark J and Luo, Liqun",
  abstract = "The human brain contains approximately 60 billion cerebellar
              granule cells, which outnumber all other brain neurons combined.
              Classical theories posit that a large, diverse population of
              granule cells allows for highly detailed representations of
              sensorimotor context, enabling downstream Purkinje cells to sense
              fine contextual changes. Although evidence suggests a role for
              the cerebellum in cognition, granule cells are known to encode
              only sensory and motor context. Here, using two-photon calcium
              imaging in behaving mice, we show that granule cells convey
              information about the expectation of reward. Mice initiated
              voluntary forelimb movements for delayed sugar-water reward. Some
              granule cells responded preferentially to reward or reward
              omission, whereas others selectively encoded reward anticipation.
              Reward responses were not restricted to forelimb movement, as a
              Pavlovian task evoked similar responses. Compared to predictable
              rewards, unexpected rewards elicited markedly different granule
              cell activity despite identical stimuli and licking responses. In
              both tasks, reward signals were widespread throughout multiple
              cerebellar lobules. Tracking the same granule cells over several
              days of learning revealed that cells with reward-anticipating
              responses emerged from those that responded at the start of
              learning to reward delivery, whereas reward-omission responses
              grew stronger as learning progressed. The discovery of
              predictive, non-sensorimotor encoding in granule cells is a major
              departure from the current understanding of these neurons and
              markedly enriches the contextual information available to
              postsynaptic Purkinje cells, with important implications for
              cognitive processing in the cerebellum.",
  journal  = "Nature",
  volume   =  544,
  number   =  7648,
  pages    = "96--100",
  year     =  2017,
  language = "en"
}

@ARTICLE{Roesch2007-uv,
  title     = "Dopamine neurons encode the better option in rats deciding
               between differently delayed or sized rewards",
  author    = "Roesch, Matthew R and Calu, Donna J and Schoenbaum, Geoffrey",
  abstract  = "The dopamine system is thought to be involved in making
               decisions about reward. Here we recorded from the ventral
               tegmental area in rats learning to choose between differently
               delayed and sized rewards. As expected, the activity of many
               putative dopamine neurons reflected reward prediction errors,
               changing when the value of the reward increased or decreased
               unexpectedly. During learning, neural responses to reward in
               these neurons waned and responses to cues that predicted reward
               emerged. Notably, this cue-evoked activity varied with size and
               delay. Moreover, when rats were given a choice between two
               differently valued outcomes, the activity of the neurons
               initially reflected the more valuable option, even when it was
               not subsequently selected.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  10,
  number    =  12,
  pages     = "1615--1624",
  year      =  2007,
  language  = "en"
}

@ARTICLE{Threlfell2012-sr,
  title     = "Striatal dopamine release is triggered by synchronized activity
               in cholinergic interneurons",
  author    = "Threlfell, Sarah and Lalic, Tatjana and Platt, Nicola J and
               Jennings, Katie A and Deisseroth, Karl and Cragg, Stephanie J",
  abstract  = "Striatal dopamine plays key roles in our normal and pathological
               goal-directed actions. To understand dopamine function, much
               attention has focused on how midbrain dopamine neurons modulate
               their firing patterns. However, we identify a presynaptic
               mechanism that triggers dopamine release directly, bypassing
               activity in dopamine neurons. We paired electrophysiological
               recordings of striatal channelrhodopsin2-expressing cholinergic
               interneurons with simultaneous detection of dopamine release at
               carbon-fiber microelectrodes in striatal slices. We reveal that
               activation of cholinergic interneurons by light flashes that
               cause only single action potentials in neurons from a small
               population triggers dopamine release via activation of nicotinic
               receptors on dopamine axons. This event overrides ascending
               activity from dopamine neurons and, furthermore, is reproduced
               by activating ChR2-expressing thalamostriatal inputs, which
               synchronize cholinergic interneurons in vivo. These findings
               indicate that synchronized activity in cholinergic interneurons
               directly generates striatal dopamine signals whose functions
               will extend beyond those encoded by dopamine neuron activity.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  75,
  number    =  1,
  pages     = "58--64",
  year      =  2012,
  language  = "en"
}

@BOOK{Hohwy2013-ga,
  title     = "The Predictive Mind",
  author    = "Hohwy, Jakob",
  abstract  = "A new theory is taking hold in neuroscience. It is the theory
               that the brain is essentially a hypothesis-testing mechanism,
               one that attempts to minimise the error of its predictions about
               the sensory input it receives from the world. It is an
               attractive theory because powerful theoretical arguments support
               it, and yet it is at heart stunningly simple. Jakob Hohwy
               explains and explores this theory from the perspective of
               cognitive science and philosophy. The key argument throughout
               The Predictive Mind is that the mechanism explains the rich,
               deep, and multifaceted character of our conscious perception. It
               also gives a unified account of how perception is sculpted by
               attention, and how it depends on action. The mind is revealed as
               having a fragile and indirect relation to the world. Though we
               are deeply in tune with the world we are also strangely
               distanced from it. The first part of the book sets out how the
               theory enables rich, layered perception. The theory's
               probabilistic and statistical foundations are explained using
               examples from empirical research and analogies to different
               forms of inference. The second part uses the simple mechanism in
               an explanation of problematic cases of how we manage to
               represent, and sometimes misrepresent, the world in health as
               well as in mental illness. The third part looks into the mind,
               and shows how the theory accounts for attention, conscious
               unity, introspection, self and the privacy of our mental world.",
  publisher = "Oxford University Press",
  year      =  2013,
  language  = "en"
}

@ARTICLE{Duan2016-yi,
  title         = "{RL$^2$}: Fast Reinforcement Learning via Slow Reinforcement
                   Learning",
  author        = "Duan, Yan and Schulman, John and Chen, Xi and Bartlett,
                   Peter L and Sutskever, Ilya and Abbeel, Pieter",
  abstract      = "Deep reinforcement learning (deep RL) has been successful in
                   learning sophisticated behaviors automatically; however, the
                   learning process requires a huge number of trials. In
                   contrast, animals can learn new tasks in just a few trials,
                   benefiting from their prior knowledge about the world. This
                   paper seeks to bridge this gap. Rather than designing a
                   ``fast'' reinforcement learning algorithm, we propose to
                   represent it as a recurrent neural network (RNN) and learn
                   it from data. In our proposed method, RL$^2$, the algorithm
                   is encoded in the weights of the RNN, which are learned
                   slowly through a general-purpose (``slow'') RL algorithm.
                   The RNN receives all information a typical RL algorithm
                   would receive, including observations, actions, rewards, and
                   termination flags; and it retains its state across episodes
                   in a given Markov Decision Process (MDP). The activations of
                   the RNN store the state of the ``fast'' RL algorithm on the
                   current (previously unseen) MDP. We evaluate RL$^2$
                   experimentally on both small-scale and large-scale problems.
                   On the small-scale side, we train it to solve randomly
                   generated multi-arm bandit problems and finite MDPs. After
                   RL$^2$ is trained, its performance on new MDPs is close to
                   human-designed algorithms with optimality guarantees. On the
                   large-scale side, we test RL$^2$ on a vision-based
                   navigation task and show that it scales up to
                   high-dimensional problems.",
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1611.02779"
}


@ARTICLE{Badcock2018-xh,
  title    = "The Hierarchically Mechanistic Mind: A {Free-Energy} Formulation
              of the Human Psyche",
  author   = "Badcock, Paul and Friston, Karl J and Ramstead, Maxwell J D",
  abstract = "PDF | This article presents a unifying theory of the embodied,
              situated human brain called the Hierarchically Mechanistic Mind
              (HMM). The HMM describes the brain as a complex adaptive system
              that actively minimises the decay of our sensory and physical
              states by producing adaptive...",
  journal  =  {Physics of life Reviews},
  year     =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kaelbling1996-bb,
  title     = "Reinforcement Learning: A Survey",
  author    = "Kaelbling, L P and Littman, M L and Moore, A W",
  abstract  = "This paper surveys the field of reinforcement learning from a
               computer-science perspective. It is written to be accessible to
               researchers familiar with machine learning. Both the historical
               basis of the field and a broad selection of current work are
               summarized. Reinforcement …",
  journal   = "1",
  publisher = "jair.org",
  volume    =  4,
  pages     = "237--285",
  year      =  1996
}

@ARTICLE{Van_Essen1992-uo,
  title    = "Information processing in the primate visual system: an
              integrated systems perspective",
  author   = "Van Essen, D C and Anderson, C H and Felleman, D J",
  abstract = "The primate visual system contains dozens of distinct areas in
              the cerebral cortex and several major subcortical structures.
              These subdivisions are extensively interconnected in a
              distributed hierarchical network that contains several
              intertwined processing streams. A number of strategies are used
              for efficient information processing within this hierarchy. These
              include linear and nonlinear filtering, passage through
              information bottlenecks, and coordinated use of multiple types of
              information. In addition, dynamic regulation of information flow
              within and between visual areas may provide the computational
              flexibility needed for the visual system to perform a broad
              spectrum of tasks accurately and at high resolution.",
  journal  = "Science",
  volume   =  255,
  number   =  5043,
  pages    = "419--423",
  month    =  jan,
  year     =  1992,
  language = "en"
}

@ARTICLE{Brimblecombe2015-eh,
  title     = "Substance {P} Weights Striatal Dopamine Transmission Differently
               within the {Striosome-Matrix} Axis",
  author    = "Brimblecombe, Katherine R and Cragg, Stephanie J",
  abstract  = "The mammalian striatum has a topographical organization of
               input-output connectivity, but a complex internal, nonlaminar
               neuronal architecture comprising projection neurons of two types
               interspersed among multiple interneuron types and potential
               local neuromodulators. From this cellular melange arises a
               biochemical compartmentalization of areas termed striosomes and
               extrastriosomal matrix. The functions of these compartments are
               poorly understood but might confer distinct features to striatal
               signal processing and be discretely governed. Dopamine
               transmission occurs throughout striosomes and matrix, and is
               reported to be modulated by the striosomally enriched
               neuromodulator substance P. However, reported effects are
               conflicting, ranging from facilitation to inhibition. We
               addressed whether dopamine transmission is modulated differently
               in striosome-matrix compartments by substance P.We paired
               detection of evoked dopamine release at carbon-fiber
               microelectrodes in mouse striatal slices with subsequent
               identification of the location of recording sites with respect
               to $\mu$-opioid receptor-rich striosomes. Substance P had
               bidirectional effects on dopamine release that varied between
               recording sites and were prevented by inhibition of neurokinin-1
               receptors. The direction of modulation was determined by
               location within the striosomal-matrix axis: dopamine release was
               boosted in striosome centers, diminished in striosomal-matrix
               border regions, and unaffected in the matrix. In turn, this
               different weighting of dopamine transmission by substance P
               modified the apparent center-surround contrast of striosomal
               dopamine signals. These data reveal that dopamine transmission
               can be differentially modulated within the striosomal-matrix
               axis, and furthermore, indicate a functionally distinct zone at
               the striosome-matrix interface, which may have key impacts on
               striatal integration.",
  journal   = "J. Neurosci.",
  publisher = "Soc Neuroscience",
  volume    =  35,
  number    =  24,
  pages     = "9017--9023",
  month     =  jun,
  year      =  2015,
  keywords  = "NK1 receptors; dopamine; striatum; striosomes; substance P;
               voltammetry",
  language  = "en"
}

@ARTICLE{Niv2019-xg,
  title    = "Learning task-state representations",
  author   = "Niv, Yael",
  abstract = "Arguably, the most difficult part of learning is deciding what to
              learn about. Should I associate the positive outcome of safely
              completing a street-crossing with the situation 'the car
              approaching the crosswalk was red' or with 'the approaching car
              was slowing down'? In this Perspective, we summarize our recent
              research into the computational and neural underpinnings of
              'representation learning'-how humans (and other animals)
              construct task representations that allow efficient learning and
              decision-making. We first discuss the problem of learning what to
              ignore when confronted with too much information, so that
              experience can properly generalize across situations. We then
              turn to the problem of augmenting perceptual information with
              inferred latent causes that embody unobservable task-relevant
              information, such as contextual knowledge. Finally, we discuss
              recent findings regarding the neural substrates of task
              representations that suggest the orbitofrontal cortex represents
              'task states', deploying them for decision-making and learning
              elsewhere in the brain.",
  journal  = "Nat. Neurosci.",
  volume   =  22,
  number   =  10,
  pages    = "1544--1553",
  year     =  2019,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Barto1995-kn,
  title     = "Adaptive critics and the basal ganglia",
  author    = "Barto, Andrew G",
  abstract  = "One of the most active areas of research in artificial
               intelilgence is the study of learning methods by which``
               embedded agents'' can improve performance while acting in
               complex dynamic environments. An agent, or decision maker, is
               embedded in an environment when …",
  journal   = "Models of information processing in the basal ganglia",
  publisher = "MIT press",
  pages     = "215",
  year      =  1995
}

@ARTICLE{Gershman2018-ac,
  title    = "Deconstructing the human algorithms for exploration",
  author   = "Gershman, Samuel J",
  abstract = "The dilemma between information gathering (exploration) and
              reward seeking (exploitation) is a fundamental problem for
              reinforcement learning agents. How humans resolve this dilemma is
              still an open question, because experiments have provided
              equivocal evidence about the underlying algorithms used by
              humans. We show that two families of algorithms can be
              distinguished in terms of how uncertainty affects exploration.
              Algorithms based on uncertainty bonuses predict a change in
              response bias as a function of uncertainty, whereas algorithms
              based on sampling predict a change in response slope. Two
              experiments provide evidence for both bias and slope changes, and
              computational modeling confirms that a hybrid model is the best
              quantitative account of the data.",
  journal  = "Cognition",
  volume   =  173,
  pages    = "34--42",
  month    =  apr,
  year     =  2018,
  keywords = "Bayesian inference; Explore-exploit dilemma; Reinforcement
              learning",
  language = "en"
}

@ARTICLE{Barbey2011-ec,
  title    = "Orbitofrontal contributions to human working memory",
  author   = "Barbey, Aron K and Koenigs, Michael and Grafman, Jordan",
  abstract = "Although cognitive neuroscience has made remarkable progress in
              understanding the involvement of the prefrontal cortex in human
              memory, the necessity of the orbitofrontal cortex for key
              competencies of working memory remains largely unexplored. We
              therefore studied human brain lesion patients to determine
              whether the orbitofrontal cortex is necessary for working memory
              function, administering subtests of the Wechsler memory scale,
              the Wechsler adult intelligence scale, and the n-back task to 3
              participant groups: orbitofrontal lesions (n = 24), prefrontal
              lesions not involving orbitofrontal cortex (n = 40), and no brain
              lesions (n = 54). Orbitofrontal damage was reliably associated
              with deficits on neuropsychological tests involving the
              coordination of working memory maintenance, manipulation, and
              monitoring processes (n-back task) but not on pure tests of
              working memory maintenance (digit/spatial span forward) or
              manipulation (digit/spatial span backward and letter-number
              sequencing). Our findings elucidate a central component of the
              neural architecture of working memory, providing key
              neuropsychological evidence for the necessity of the
              orbitofrontal cortex in executive control functions underlying
              the joint maintenance, manipulation, and monitoring of
              information in working memory.",
  journal  = "Cereb. Cortex",
  volume   =  21,
  number   =  4,
  pages    = "789--795",
  month    =  apr,
  year     =  2011,
  language = "en"
}

@ARTICLE{Redgrave1999-jp,
  title     = "The basal ganglia: a vertebrate solution to the selection
               problem?",
  author    = "Redgrave, P and Prescott, T J and Gurney, K",
  abstract  = "A selection problem arises whenever two or more competing
               systems seek simultaneous access to a restricted resource.
               Consideration of several selection architectures suggests there
               are significant advantages for systems which incorporate a
               central switching mechanism. We propose that the vertebrate
               basal ganglia have evolved as a centralized selection device,
               specialized to resolve conflicts over access to limited motor
               and cognitive resources. Analysis of basal ganglia functional
               architecture and its position within a wider anatomical
               framework suggests it can satisfy many of the requirements
               expected of an efficient selection mechanism.",
  journal   = "Neuroscience",
  publisher = "eprints.whiterose.ac.uk",
  volume    =  89,
  number    =  4,
  pages     = "1009--1023",
  year      =  1999,
  language  = "en"
}

@ARTICLE{Wunderlich2012-mo,
  title    = "Mapping value based planning and extensively trained choice in
              the human brain",
  author   = "Wunderlich, Klaus and Dayan, Peter and Dolan, Raymond J",
  abstract = "Investigations of the underlying mechanisms of choice in humans
              have focused on learning from prediction errors, leaving the
              computational structure of value based planning comparatively
              underexplored. Using behavioral and neuroimaging analyses of a
              minimax decision task, we found that the computational processes
              underlying forward planning are expressed in the anterior caudate
              nucleus as values of individual branching steps in a decision
              tree. In contrast, values represented in the putamen pertain
              solely to values learned during extensive training. During actual
              choice, both striatal areas showed a functional coupling to
              ventromedial prefrontal cortex, consistent with this region
              acting as a value comparator. Our findings point toward an
              architecture of choice in which segregated value systems operate
              in parallel in the striatum for planning and extensively trained
              choices, with medial prefrontal cortex integrating their outputs.",
  journal  = "Nat. Neurosci.",
  volume   =  15,
  number   =  5,
  pages    = "786--791",
  month    =  mar,
  year     =  2012,
  language = "en"
}

@ARTICLE{Berke2018-jb,
  title     = "What does dopamine mean?",
  author    = "Berke, Joshua D",
  abstract  = "Dopamine is a critical modulator of both learning and
               motivation. This presents a problem: how can target cells know
               whether increased dopamine is a signal to learn or to move? It
               is often presumed that motivation involves slow (`tonic')
               dopamine changes, while fast (`phasic') dopamine fluctuations
               convey reward prediction errors for learning. Yet recent studies
               have shown that dopamine conveys motivational value and promotes
               movement even on subsecond timescales. Here I describe an
               alternative account of how dopamine regulates ongoing behavior.
               Dopamine release related to motivation is rapidly and locally
               sculpted by receptors on dopamine terminals, independently from
               dopamine cell firing. Target neurons abruptly switch between
               learning and performance modes, with striatal cholinergic
               interneurons providing one candidate switch mechanism. The
               behavioral impact of dopamine varies by subregion, but in each
               case dopamine provides a dynamic estimate of whether it is worth
               expending a limited internal resource, such as energy,
               attention, or time.",
  journal   = "Nat. Neurosci.",
  publisher = "Nature Publishing Group",
  volume    =  21,
  number    =  6,
  pages     = "787--793",
  month     =  may,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Niv2015-gg,
  title    = "Reinforcement learning in multidimensional environments relies on
              attention mechanisms",
  author   = "Niv, Yael and Daniel, Reka and Geana, Andra and Gershman, Samuel
              J and Leong, Yuan Chang and Radulescu, Angela and Wilson, Robert
              C",
  abstract = "In recent years, ideas from the computational field of
              reinforcement learning have revolutionized the study of learning
              in the brain, famously providing new, precise theories of how
              dopamine affects learning in the basal ganglia. However,
              reinforcement learning algorithms are notorious for not scaling
              well to multidimensional environments, as is required for
              real-world learning. We hypothesized that the brain naturally
              reduces the dimensionality of real-world problems to only those
              dimensions that are relevant to predicting reward, and conducted
              an experiment to assess by what algorithms and with what neural
              mechanisms this ``representation learning'' process is realized
              in humans. Our results suggest that a bilateral attentional
              control network comprising the intraparietal sulcus, precuneus,
              and dorsolateral prefrontal cortex is involved in selecting what
              dimensions are relevant to the task at hand, effectively updating
              the task representation through trial and error. In this way,
              cortical attention mechanisms interact with learning in the basal
              ganglia to solve the ``curse of dimensionality'' in reinforcement
              learning.",
  journal  = "J. Neurosci.",
  volume   =  35,
  number   =  21,
  pages    = "8145--8157",
  month    =  may,
  year     =  2015,
  keywords = "attention; fMRI; frontoparietal network; model comparison;
              reinforcement learning; representation learning",
  language = "en"
}

@ARTICLE{Chuhma2014-dn,
  title     = "Dopamine neurons control striatal cholinergic neurons via
               regionally heterogeneous dopamine and glutamate signaling",
  author    = "Chuhma, Nao and Mingote, Susana and Moore, Holly and Rayport,
               Stephen",
  abstract  = "Midbrain dopamine neurons fire in bursts conveying salient
               information. Bursts are associated with pauses in tonic firing
               of striatal cholinergic interneurons. Although the reciprocal
               balance of dopamine and acetylcholine in the striatum is well
               known, how dopamine neurons control cholinergic neurons has not
               been elucidated. Here, we show that dopamine neurons make direct
               fast dopaminergic and glutamatergic connections with cholinergic
               interneurons, with regional heterogeneity. Dopamine neurons
               drive a burst-pause firing sequence in cholinergic interneurons
               in the medial shell of the nucleus accumbens, mixed actions in
               the accumbens core, and a pause in the dorsal striatum. This
               heterogeneity is due mainly to regional variation in
               dopamine-neuron glutamate cotransmission. A single dose of
               amphetamine attenuates dopamine neuron connections to
               cholinergic interneurons with dose-dependent regional
               specificity. Overall, the present data indicate that dopamine
               neurons control striatal circuit function via discrete, plastic
               connections with cholinergic interneurons.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  81,
  number    =  4,
  pages     = "901--912",
  month     =  feb,
  year      =  2014,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Miller1956-ok,
  title     = "The magical number seven plus or minus two: some limits on our
               capacity for processing information",
  author    = "Miller, G A",
  abstract  = "A variety of researches are examined from the standpoint of
               information theory. It is shown that the unaided observer is
               severely limited in terms of the amount of information he can
               receive, process, and remember. However, it is shown that by the
               use of various techniques …",
  journal   = "Psychol. Rev.",
  publisher = "psycnet.apa.org",
  volume    =  63,
  number    =  2,
  pages     = "81--97",
  month     =  mar,
  year      =  1956,
  keywords  = "PSYCHOMETRICS; STATISTICS",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Herrnstein1961-bf,
  title     = "Relative and absolute strength of response as a function of
               frequency of reinforcement",
  author    = "Herrnstein, R J",
  abstract  = "A previous paper (Herrnstein, 1958) reported how pigeons behave
               on a concurrent schedule under which they peck at either of two
               response-keys. The significant finding of this investigation was
               that the relative frequency of responding to each of the keys
               may be controlled within narrow limits by adjustments in an
               in-dependent variable. In brief, the requirement for
               rein-forcement in this procedure is the emission of a minimum
               number of pecks to each of the keys. The pigeon receives food
               when it completes the requirement on …",
  journal   = "J. Exp. Anal. Behav.",
  publisher = "Wiley Online Library",
  volume    =  4,
  pages     = "267--272",
  month     =  jul,
  year      =  1961,
  keywords  = "LEARNING",
  language  = "en"
}

@INCOLLECTION{Schuck2018-ik,
  title     = "Chapter 12 - A State Representation for Reinforcement Learning
               and {Decision-Making} in the Orbitofrontal Cortex",
  booktitle = "{Goal-Directed} Decision Making",
  author    = "Schuck, Nicolas W and Wilson, Robert and Niv, Yael",
  editor    = "Morris, Richard and Bornstein, Aaron and Shenhav, Amitai",
  abstract  = "Despite decades of research, the exact ways in which the
               orbitofrontal cortex (OFC) influences cognitive function have
               remained mysterious. Anatomically, the OFC is characterized by
               remarkably broad connectivity to sensory, limbic, and
               subcortical areas, and functional studies have implicated the
               OFC in a plethora of functions ranging from facial processing to
               value-guided choice. Notwithstanding such diversity of findings,
               much research suggests that one important function of the OFC is
               to support decision-making and reinforcement learning. Here, we
               describe a novel theory that posits that OFC's specific role in
               decision-making is to provide an up-to-date representation of
               task-related information, called a state representation. This
               representation reflects a mapping between distinct task states
               and sensory as well as unobservable information. We summarize
               evidence supporting the existence of such state representations
               in rodent and human OFC and argue that forming these state
               representations provides a crucial scaffold that allows animals
               to efficiently perform decision-making and reinforcement
               learning in high-dimensional and partially observable
               environments. Finally, we argue that our theory offers an
               integrating framework for linking the diversity of functions
               ascribed to OFC and is in line with its wide ranging
               connectivity.",
  publisher = "Academic Press",
  pages     = "259--278",
  year      =  2018,
  keywords  = "Decision-making; Orbitofrontal cortex; Reinforcement learning"
}

@ARTICLE{Balleine2010-wt,
  title     = "Human and rodent homologies in action control: corticostriatal
               determinants of goal-directed and habitual action",
  author    = "Balleine, Bernard W and O'Doherty, John P",
  abstract  = "Recent behavioral studies in both humans and rodents have found
               evidence that performance in decision-making tasks depends on
               two different learning processes; one encoding the relationship
               between actions and their consequences and a second involving
               the formation of stimulus-response associations. These learning
               processes are thought to govern goal-directed and habitual
               actions, respectively, and have been found to depend on
               homologous corticostriatal networks in these species. Thus,
               recent research using comparable behavioral tasks in both humans
               and rats has implicated homologous regions of cortex (medial
               prefrontal cortex/medial orbital cortex in humans and prelimbic
               cortex in rats) and of dorsal striatum (anterior caudate in
               humans and dorsomedial striatum in rats) in goal-directed action
               and in the control of habitual actions (posterior lateral
               putamen in humans and dorsolateral striatum in rats). These
               learning processes have been argued to be antagonistic or
               competing because their control over performance appears to be
               all or none. Nevertheless, evidence has started to accumulate
               suggesting that they may at times compete and at others
               cooperate in the selection and subsequent evaluation of actions
               necessary for normal choice performance. It appears likely that
               cooperation or competition between these sources of action
               control depends not only on local interactions in dorsal
               striatum but also on the cortico-basal ganglia network within
               which the striatum is embedded and that mediates the integration
               of learning with basic motivational and emotional processes. The
               neural basis of the integration of learning and motivation in
               choice and decision-making is still controversial and we review
               some recent hypotheses relating to this issue.",
  journal   = "Neuropsychopharmacology",
  publisher = "nature.com",
  volume    =  35,
  number    =  1,
  pages     = "48--69",
  month     =  jan,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Friston2010-wi,
  title    = "The free-energy principle: a unified brain theory?",
  author   = "Friston, Karl",
  abstract = "A free-energy principle has been proposed recently that accounts
              for action, perception and learning. This Review looks at some
              key brain theories in the biological (for example, neural
              Darwinism) and physical (for example, information theory and
              optimal control theory) sciences from the free-energy
              perspective. Crucially, one key theme runs through each of these
              theories - optimization. Furthermore, if we look closely at what
              is optimized, the same quantity keeps emerging, namely value
              (expected reward, expected utility) or its complement, surprise
              (prediction error, expected cost). This is the quantity that is
              optimized under the free-energy principle, which suggests that
              several global brain theories might be unified within a
              free-energy framework.",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  11,
  number   =  2,
  pages    = "127--138",
  year     =  2010,
  language = "en"
}

@ARTICLE{Schulman2017-ph,
  title         = "Proximal Policy Optimization Algorithms",
  author        = "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and
                   Radford, Alec and Klimov, Oleg",
  abstract      = "We propose a new family of policy gradient methods for
                   reinforcement learning, which alternate between sampling
                   data through interaction with the environment, and
                   optimizing a ``surrogate'' objective function using
                   stochastic gradient ascent. Whereas standard policy gradient
                   methods perform one gradient update per data sample, we
                   propose a novel objective function that enables multiple
                   epochs of minibatch updates. The new methods, which we call
                   proximal policy optimization (PPO), have some of the
                   benefits of trust region policy optimization (TRPO), but
                   they are much simpler to implement, more general, and have
                   better sample complexity (empirically). Our experiments test
                   PPO on a collection of benchmark tasks, including simulated
                   robotic locomotion and Atari game playing, and we show that
                   PPO outperforms other online policy gradient methods, and
                   overall strikes a favorable balance between sample
                   complexity, simplicity, and wall-time.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1707.06347"
}

@ARTICLE{Conway2000-bl,
  title    = "The construction of autobiographical memories in the self-memory
              system",
  author   = "Conway, M A and Pleydell-Pearce, C W",
  abstract = "The authors describe a model of autobiographical memory in which
              memories are transitory mental constructions within a self-memory
              system (SMS). The SMS contains an autobiographical knowledge base
              and current goals of the working self. Within the SMS, control
              processes modulate access to the knowledge base by successively
              shaping cues used to activate autobiographical memory knowledge
              structures and, in this way, form specific memories. The relation
              of the knowledge base to active goals is reciprocal, and the
              knowledge base ``grounds'' the goals of the working self. It is
              shown how this model can be used to draw together a wide range of
              diverse data from cognitive, social, developmental, personality,
              clinical, and neuropsychological autobiographical memory
              research.",
  journal  = "Psychol. Rev.",
  volume   =  107,
  number   =  2,
  pages    = "261--288",
  month    =  apr,
  year     =  2000,
  language = "en"
}

@ARTICLE{Dayan1993-ve,
  title     = "Improving Generalization for Temporal Difference Learning: The
               Successor Representation",
  author    = "Dayan, Peter",
  abstract  = "Estimation of returns over time, the focus of temporal
               difference (TD) algorithms, imposes particular constraints on
               good function approximators or representations. Appropriate
               generalization between states is determined by how similar their
               successors are, and representations should follow suit. This
               paper shows how TD machinery can be used to learn such
               representations, and illustrates, using a navigation task, the
               appropriately distributed nature of the result.",
  journal   = "Neural Comput.",
  publisher = "MIT Press",
  volume    =  5,
  number    =  4,
  pages     = "613--624",
  month     =  jul,
  year      =  1993
}

@ARTICLE{Gershman2017-ah,
  title    = "Reinforcement Learning and Episodic Memory in Humans and Animals:
              An Integrative Framework",
  author   = "Gershman, Samuel J and Daw, Nathaniel D",
  abstract = "We review the psychology and neuroscience of reinforcement
              learning (RL), which has experienced significant progress in the
              past two decades, enabled by the comprehensive experimental study
              of simple learning and decision-making tasks. However, one
              challenge in the study of RL is computational: The simplicity of
              these tasks ignores important aspects of reinforcement learning
              in the real world: (a) State spaces are high-dimensional,
              continuous, and partially observable; this implies that (b) data
              are relatively sparse and, indeed, precisely the same situation
              may never be encountered twice; furthermore, (c) rewards depend
              on the long-term consequences of actions in ways that violate the
              classical assumptions that make RL tractable. A seemingly
              distinct challenge is that, cognitively, theories of RL have
              largely involved procedural and semantic memory, the way in which
              knowledge about action values or world models extracted gradually
              from many experiences can drive choice. This focus on semantic
              memory leaves out many aspects of memory, such as episodic
              memory, related to the traces of individual events. We suggest
              that these two challenges are related. The computational
              challenge can be dealt with, in part, by endowing RL systems with
              episodic memory, allowing them to (a) efficiently approximate
              value functions over complex state spaces, (b) learn with very
              little data, and (c) bridge long-term dependencies between
              actions and rewards. We review the computational theory
              underlying this proposal and the empirical evidence to support
              it. Our proposal suggests that the ubiquitous and diverse roles
              of memory in RL may function as part of an integrated learning
              system.",
  journal  = "Annu. Rev. Psychol.",
  volume   =  68,
  pages    = "101--128",
  year     =  2017,
  keywords = "decision making; memory; reinforcement learning",
  language = "en"
}

@ARTICLE{Collins2016-lz,
  title    = "Surprise! Dopamine signals mix action, value and error",
  author   = "Collins, Anne G E and Frank, Michael J",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  1,
  pages    = "3--5",
  year     =  2016,
  language = "en"
}

@ARTICLE{Hamid2016-tk,
  title    = "Mesolimbic dopamine signals the value of work",
  author   = "Hamid, Arif A and Pettibone, Jeffrey R and Mabrouk, Omar S and
              Hetrick, Vaughn L and Schmidt, Robert and Vander Weele, Caitlin M
              and Kennedy, Robert T and Aragona, Brandon J and Berke, Joshua D",
  abstract = "Dopamine cell firing can encode errors in reward prediction,
              providing a learning signal to guide future behavior. Yet
              dopamine is also a key modulator of motivation, invigorating
              current behavior. Existing theories propose that fast (phasic)
              dopamine fluctuations support learning, whereas much slower
              (tonic) dopamine changes are involved in motivation. We examined
              dopamine release in the nucleus accumbens across multiple time
              scales, using complementary microdialysis and voltammetric
              methods during adaptive decision-making. We found that
              minute-by-minute dopamine levels covaried with reward rate and
              motivational vigor. Second-by-second dopamine release encoded an
              estimate of temporally discounted future reward (a value
              function). Changing dopamine immediately altered willingness to
              work and reinforced preceding action choices by encoding
              temporal-difference reward prediction errors. Our results
              indicate that dopamine conveys a single, rapidly evolving
              decision variable, the available reward for investment of effort,
              which is employed for both learning and motivational functions.",
  journal  = "Nat. Neurosci.",
  volume   =  19,
  number   =  1,
  pages    = "117--126",
  month    =  jan,
  year     =  2016,
  language = "en"
}

@ARTICLE{Schultz2017-yb,
  title     = "The phasic dopamine signal maturing: from reward via behavioural
               activation to formal economic utility",
  author    = "Schultz, Wolfram and Stauffer, Wiliam R and Lak, Armin",
  abstract  = "The phasic dopamine reward prediction error response is a major
               brain signal underlying learning, approach and decision making.
               This dopamine response consists of two components that reflect,
               initially, stimulus detection from physical impact and,
               subsequenttly, reward valuation; dopamine activations by
               punishers reflect physical impact rather than aversiveness. The
               dopamine reward signal is distinct from earlier reported and
               recently confirmed phasic changes with behavioural activation.
               Optogenetic activation of dopamine neurones in monkeys causes
               value learning and biases economic choices. The dopamine reward
               signal conforms to formal economic utility and thus constitutes
               a utility prediction error signal. In these combined ways, the
               dopamine reward prediction error signal constitutes a potential
               neuronal substrate for the crucial economic decision variable of
               utility.",
  journal   = "Curr. Opin. Neurobiol.",
  publisher = "Elsevier",
  volume    =  43,
  pages     = "139--148",
  month     =  apr,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Felin2017-cy,
  title    = "Rationality, perception, and the all-seeing eye",
  author   = "Felin, Teppo and Koenderink, Jan and Krueger, Joachim I",
  abstract = "Seeing-perception and vision-is implicitly the fundamental
              building block of the literature on rationality and cognition.
              Herbert Simon and Daniel Kahneman's arguments against the
              omniscience of economic agents-and the concept of bounded
              rationality-depend critically on a particular view of the nature
              of perception and vision. We propose that this framework of
              rationality merely replaces economic omniscience with perceptual
              omniscience. We show how the cognitive and social sciences
              feature a pervasive but problematic meta-assumption that is
              characterized by an ``all-seeing eye.'' We raise concerns about
              this assumption and discuss different ways in which the
              all-seeing eye manifests itself in existing research on (bounded)
              rationality. We first consider the centrality of vision and
              perception in Simon's pioneering work. We then point to
              Kahneman's work-particularly his article ``Maps of Bounded
              Rationality''-to illustrate the pervasiveness of an all-seeing
              view of perception, as manifested in the extensive use of visual
              examples and illusions. Similar assumptions about perception can
              be found across a large literature in the cognitive sciences. The
              central problem is the present emphasis on inverse optics-the
              objective nature of objects and environments, e.g., size,
              contrast, and color. This framework ignores the nature of the
              organism and perceiver. We argue instead that reality is
              constructed and expressed, and we discuss the species-specificity
              of perception, as well as perception as a user interface. We draw
              on vision science as well as the arts to develop an alternative
              understanding of rationality in the cognitive and social
              sciences. We conclude with a discussion of the implications of
              our arguments for the rationality and decision-making literature
              in cognitive psychology and behavioral economics, along with
              suggesting some ways forward.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  24,
  number   =  4,
  pages    = "1040--1059",
  month    =  aug,
  year     =  2017,
  keywords = "Cognition; Perception; Rationality; Social science",
  language = "en"
}

@MISC{Sutton_undated-sq,
  title        = "The reward hypothesis",
  author       = "Sutton, Richard",
  howpublished = "\url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}",
  note         = "Accessed: 2019-2-23"
}

@UNPUBLISHED{Elliott_Wimmer2018-ex,
  title    = "Learning of distant state predictions by the orbitofrontal cortex
              in humans",
  author   = "Elliott Wimmer, G and Buechel, Christian",
  abstract = "Representations of our future environment beyond what is
              immediately perceptible are critical for planning and decision
              making. Previous research in humans has demonstrated that the
              hippocampus is critical for forming and retrieving associations
              and that the orbitofrontal cortex - ventromedial frontal cortex
              (OFC-VMPFC) is an important region for representing information
              about recent states. However, we do not know how the brain
              acquires predictive representations during goal-directed based
              learning. When goals are more than a few seconds in the future,
              such predictive representations are critically important for
              adaptive decision making. We examined the acquisition of
              predictive information by recording brain activity with fMRI
              while participants learned to find rewards in multiple different
              Y-maze environments, where exploration of each maze lasted more
              than 40 seconds. Behaviorally, participants exhibited rapid
              acquisition of optimal choices after only one or two experiences.
              Neurally, we found that activity in the hippocampus was highest
              during initial exposure and then decayed across the remaining
              repetitions of each maze, consistent with a role in rapid
              encoding of the maze experience. Importantly, we found that
              across learning, multivariate patterns in the OFC- VPFC but not
              the hippocampus or visual cortex came to represent predictive
              information about states ~30 seconds in the future. By the end of
              learning, these representations were of similar strength as
              representations of the current state. Our findings provide a
              mechanism by which the brain can build models of world that span
              long timescales, allowing predictions to be made about distant
              future events.",
  journal  = "bioRxiv",
  pages    = "450999",
  year     =  2018,
  language = "en"
}

@ARTICLE{Schuck2016-td,
  title     = "Human Orbitofrontal Cortex Represents a Cognitive Map of State
               Space",
  author    = "Schuck, Nicolas W and Cai, Ming Bo and Wilson, Robert C and Niv,
               Yael",
  abstract  = "Although the orbitofrontal cortex (OFC) has been studied
               intensely for decades, its precise functions have remained
               elusive. We recently hypothesized that the OFC contains a
               ``cognitive map'' of task space in which the current state of
               the task is represented, and this representation is especially
               critical for behavior when states are unobservable from sensory
               input. To test this idea, we apply pattern-classification
               techniques to neuroimaging data from humans performing a
               decision-making task with 16 states. We show that unobservable
               task states can be decoded from activity in OFC, and decoding
               accuracy is related to task performance and the occurrence of
               individual behavioral errors. Moreover, similarity between the
               neural representations of consecutive states correlates with
               behavioral accuracy in corresponding state transitions. These
               results support the idea that OFC represents a cognitive map of
               task space and establish the feasibility of decoding state
               representations in humans using non-invasive neuroimaging.",
  journal   = "Neuron",
  publisher = "Elsevier",
  volume    =  91,
  number    =  6,
  pages     = "1402--1412",
  month     =  sep,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Barbey2013-pl,
  title    = "Dorsolateral prefrontal contributions to human working memory",
  author   = "Barbey, Aron K and Koenigs, Michael and Grafman, Jordan",
  abstract = "Although neuroscience has made remarkable progress in
              understanding the involvement of prefrontal cortex (PFC) in human
              memory, the necessity of dorsolateral PFC (dlPFC) for key
              competencies of working memory remains largely unexplored. We
              therefore studied human brain lesion patients to determine
              whether dlPFC is necessary for working memory function,
              administering subtests of the Wechsler Memory Scale, the Wechsler
              Adult Intelligence Scale, and the N-Back Task to three
              participant groups: dlPFC lesions (n=19), non-dlPFC lesions
              (n=152), and no brain lesions (n=54). DlPFC damage was associated
              with deficits in the manipulation of verbal and spatial
              knowledge, with left dlPFC necessary for manipulating information
              in working memory and right dlPFC critical for manipulating
              information in a broader range of reasoning contexts. Our
              findings elucidate the architecture of working memory, providing
              key neuropsychological evidence for the necessity of dlPFC in the
              manipulation of verbal and spatial knowledge.",
  journal  = "Cortex",
  volume   =  49,
  number   =  5,
  pages    = "1195--1205",
  month    =  may,
  year     =  2013,
  language = "en"
}

@BOOK{Lashley1951-yd,
  title     = "The problem of serial order in behavior",
  author    = "Lashley, Karl Spencer",
  publisher = "Bobbs-Merrill",
  volume    =  21,
  year      =  1951
}

@ARTICLE{Kool2017-xs,
  title    = "{Cost-Benefit} Arbitration Between Multiple
              {Reinforcement-Learning} Systems",
  author   = "Kool, Wouter and Gershman, Samuel J and Cushman, Fiery A",
  abstract = "Human behavior is sometimes determined by habit and other times
              by goal-directed planning. Modern reinforcement-learning theories
              formalize this distinction as a competition between a
              computationally cheap but inaccurate model-free system that gives
              rise to habits and a computationally expensive but accurate
              model-based system that implements planning. It is unclear,
              however, how people choose to allocate control between these
              systems. Here, we propose that arbitration occurs by comparing
              each system's task-specific costs and benefits. To investigate
              this proposal, we conducted two experiments showing that people
              increase model-based control when it achieves greater accuracy
              than model-free control, and especially when the rewards of
              accurate performance are amplified. In contrast, they are
              insensitive to reward amplification when model-based and
              model-free control yield equivalent accuracy. This suggests that
              humans adaptively balance habitual and planned action through
              on-line cost-benefit analysis.",
  journal  = "Psychol. Sci.",
  volume   =  28,
  number   =  9,
  pages    = "1321--1333",
  month    =  sep,
  year     =  2017,
  keywords = "cognitive control; decision making; open data; open materials;
              reinforcement learning",
  language = "en"
}

@ARTICLE{Bliss1993-kd,
  title     = "A synaptic model of memory: long-term potentiation in the
               hippocampus",
  author    = "Bliss, T V and Collingridge, G L",
  abstract  = "Long-term potentiation of synaptic transmission in the
               hippocampus is the primary experimental model for investigating
               the synaptic basis of learning and memory in vertebrates. The
               best understood form of long-term potentiation is induced by the
               activation of the N-methyl-D-aspartate receptor complex. This
               subtype of glutamate receptor endows long-term potentiation with
               Hebbian characteristics, and allows electrical events at the
               postsynaptic membrane to be transduced into chemical signals
               which, in turn, are thought to activate both pre- and
               postsynaptic mechanisms to generate a persistent increase in
               synaptic strength.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  361,
  number    =  6407,
  pages     = "31--39",
  month     =  jan,
  year      =  1993,
  language  = "en"
}

@ARTICLE{Hoffman2015-he,
  title    = "The Interface Theory of Perception",
  author   = "Hoffman, Donald D and Singh, Manish and Prakash, Chetan",
  abstract = "Perception is a product of evolution. Our perceptual systems,
              like our limbs and livers, have been shaped by natural selection.
              The effects of selection on perception can be studied using
              evolutionary games and genetic algorithms. To this end, we define
              and classify perceptual strategies and allow them to compete in
              evolutionary games in a variety of worlds with a variety of
              fitness functions. We find that veridical perceptions--strategies
              tuned to the true structure of the world--are routinely dominated
              by nonveridical strategies tuned to fitness. Veridical
              perceptions escape extinction only if fitness varies
              monotonically with truth. Thus, a perceptual strategy favored by
              selection is best thought of not as a window on truth but as akin
              to a windows interface of a PC. Just as the color and shape of an
              icon for a text file do not entail that the text file itself has
              a color or shape, so also our perceptions of space-time and
              objects do not entail (by the Invention of Space-Time Theorem)
              that objective reality has the structure of space-time and
              objects. An interface serves to guide useful actions, not to
              resemble truth. Indeed, an interface hides the truth; for someone
              editing a paper or photo, seeing transistors and firmware is an
              irrelevant hindrance. For the perceptions of H. sapiens,
              space-time is the desktop and physical objects are the icons. Our
              perceptions of space-time and objects have been shaped by natural
              selection to hide the truth and guide adaptive behaviors.
              Perception is an adaptive interface.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  22,
  number   =  6,
  pages    = "1480--1506",
  month    =  dec,
  year     =  2015,
  keywords = "Bayesian inference and parameter estimation; Evolution;
              Perceptual categorization and identification; Perceptual
              learning; Spatial cognition",
  language = "en"
}

@ARTICLE{Marcus2018-eh,
  title         = "Innateness, {AlphaZero}, and Artificial Intelligence",
  author        = "Marcus, Gary",
  abstract      = "The concept of innateness is rarely discussed in the context
                   of artificial intelligence. When it is discussed, or hinted
                   at, it is often the context of trying to reduce the amount
                   of innate machinery in a given system. In this paper, I
                   consider as a test case a recent series of papers by Silver
                   et al (Silver et al., 2017a) on AlphaGo and its successors
                   that have been presented as an argument that a ``even in the
                   most challenging of domains: it is possible to train to
                   superhuman level, without human examples or guidance'',
                   ``starting tabula rasa.'' I argue that these claims are
                   overstated, for multiple reasons. I close by arguing that
                   artificial intelligence needs greater attention to
                   innateness, and I point to some proposals about what that
                   innateness might look like.",
  year         =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1801.05667"
}

@ARTICLE{Mnih2015-dx,
  title    = "Human-level control through deep reinforcement learning",
  author   = "Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and
              Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves,
              Alex and Riedmiller, Martin and Fidjeland, Andreas K and
              Ostrovski, Georg and Petersen, Stig and Beattie, Charles and
              Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran,
              Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis",
  abstract = "The theory of reinforcement learning provides a normative
              account, deeply rooted in psychological and neuroscientific
              perspectives on animal behaviour, of how agents may optimize
              their control of an environment. To use reinforcement learning
              successfully in situations approaching real-world complexity,
              however, agents are confronted with a difficult task: they must
              derive efficient representations of the environment from
              high-dimensional sensory inputs, and use these to generalize past
              experience to new situations. Remarkably, humans and other
              animals seem to solve this problem through a harmonious
              combination of reinforcement learning and hierarchical sensory
              processing systems, the former evidenced by a wealth of neural
              data revealing notable parallels between the phasic signals
              emitted by dopaminergic neurons and temporal difference
              reinforcement learning algorithms. While reinforcement learning
              agents have achieved some successes in a variety of domains,
              their applicability has previously been limited to domains in
              which useful features can be handcrafted, or to domains with
              fully observed, low-dimensional state spaces. Here we use recent
              advances in training deep neural networks to develop a novel
              artificial agent, termed a deep Q-network, that can learn
              successful policies directly from high-dimensional sensory inputs
              using end-to-end reinforcement learning. We tested this agent on
              the challenging domain of classic Atari 2600 games. We
              demonstrate that the deep Q-network agent, receiving only the
              pixels and the game score as inputs, was able to surpass the
              performance of all previous algorithms and achieve a level
              comparable to that of a professional human games tester across a
              set of 49 games, using the same algorithm, network architecture
              and hyperparameters. This work bridges the divide between
              high-dimensional sensory inputs and actions, resulting in the
              first artificial agent that is capable of learning to excel at a
              diverse array of challenging tasks.",
  journal  = "Nature",
  volume   =  518,
  number   =  7540,
  pages    = "529--533",
  month    =  feb,
  year     =  2015,
  language = "en"
}

@ARTICLE{Rougier2005-te,
  title     = "Prefrontal cortex and flexible cognitive control: rules without
               symbols",
  author    = "Rougier, Nicolas P and Noelle, David C and Braver, Todd S and
               Cohen, Jonathan D and O'Reilly, Randall C",
  abstract  = "Human cognitive control is uniquely flexible and has been shown
               to depend on prefrontal cortex (PFC). But exactly how the
               biological mechanisms of the PFC support flexible cognitive
               control remains a profound mystery. Existing theoretical models
               have posited powerful task-specific PFC representations, but not
               how these develop. We show how this can occur when a set of
               PFC-specific neural mechanisms interact with breadth of
               experience to self organize abstract rule-like PFC
               representations that support flexible generalization in novel
               tasks. The same model is shown to apply to benchmark PFC tasks
               (Stroop and Wisconsin card sorting), accurately simulating the
               behavior of neurologically intact and frontally damaged people.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  102,
  number    =  20,
  pages     = "7338--7343",
  month     =  may,
  year      =  2005,
  language  = "en"
}

@ARTICLE{Stocco2018-fu,
  title    = "A Biologically Plausible Action Selection System for Cognitive
              Architectures: Implications of Basal Ganglia Anatomy for Learning
              and {Decision-Making} Models",
  author   = "Stocco, Andrea",
  abstract = "Several attempts have been made previously to provide a
              biological grounding for cognitive architectures by relating
              their components to the computations of specific brain circuits.
              Often, the architecture's action selection system is identified
              with the basal ganglia. However, this identification overlooks
              one of the most important features of the basal ganglia-the
              existence of a direct and an indirect pathway that compete
              against each other. This characteristic has important
              consequences in decision-making tasks, which are brought to light
              by Parkinson's disease as well as genetic differences in dopamine
              receptors. This paper shows that a standard model of action
              selection in a cognitive architecture (ACT-R) cannot replicate
              any of these findings, details an alternative solution that
              reconciles action selection in the architecture with the
              physiology of the basal ganglia, and extends the domain of
              application of cognitive architectures. The implication of this
              solution for other architectures and existing models are
              discussed.",
  journal  = "Cogn. Sci.",
  volume   =  42,
  number   =  2,
  pages    = "457--490",
  month    =  mar,
  year     =  2018,
  keywords = "Basal ganglia; Cognitive architectures; Decision making;
              Neurophysiology; Production systems; Reinforcement learning",
  language = "en"
}

@ARTICLE{Pearl2018-yh,
  title         = "Theoretical Impediments to Machine Learning With Seven
                   Sparks from the Causal Revolution",
  author        = "Pearl, Judea",
  abstract      = "Current machine learning systems operate, almost
                   exclusively, in a statistical, or model-free mode, which
                   entails severe theoretical limits on their power and
                   performance. Such systems cannot reason about interventions
                   and retrospection and, therefore, cannot serve as the basis
                   for strong AI. To achieve human level intelligence, learning
                   machines need the guidance of a model of reality, similar to
                   the ones used in causal inference tasks. To demonstrate the
                   essential role of such models, I will present a summary of
                   seven tasks which are beyond reach of current machine
                   learning systems and which have been accomplished using the
                   tools of causal modeling.",
  month         =  jan,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1801.04016"
}

@ARTICLE{Lau2005-sq,
  title    = "Dynamic response-by-response models of matching behavior in
              rhesus monkeys",
  author   = "Lau, Brian and Glimcher, Paul W",
  abstract = "We studied the choice behavior of 2 monkeys in a discrete-trial
              task with reinforcement contingencies similar to those Herrnstein
              (1961) used when he described the matching law. In each session,
              the monkeys experienced blocks of discrete trials at different
              relative-reinforcer frequencies or magnitudes with unsignalled
              transitions between the blocks. Steady-state data following
              adjustment to each transition were well characterized by the
              generalized matching law; response ratios undermatched reinforcer
              frequency ratios but matched reinforcer magnitude ratios. We
              modelled response-by-response behavior with linear models that
              used past reinforcers as well as past choices to predict the
              monkeys' choices on each trial. We found that more recently
              obtained reinforcers more strongly influenced choice behavior.
              Perhaps surprisingly, we also found that the monkeys' actions
              were influenced by the pattern of their own past choices. It was
              necessary to incorporate both past reinforcers and past choices
              in order to accurately capture steady-state behavior as well as
              the fluctuations during block transitions and the
              response-by-response patterns of behavior. Our results suggest
              that simple reinforcement learning models must account for the
              effects of past choices to accurately characterize behavior in
              this task, and that models with these properties provide a
              conceptual tool for studying how both past reinforcers and past
              choices are integrated by the neural systems that generate
              behavior.",
  journal  = "J. Exp. Anal. Behav.",
  volume   =  84,
  number   =  3,
  pages    = "555--579",
  month    =  nov,
  year     =  2005,
  language = "en"
}

@ARTICLE{Simon2011-mg,
  title    = "Neural correlates of forward planning in a spatial decision task
              in humans",
  author   = "Simon, Dylan Alexander and Daw, Nathaniel D",
  abstract = "Although reinforcement learning (RL) theories have been
              influential in characterizing the mechanisms for reward-guided
              choice in the brain, the predominant temporal difference (TD)
              algorithm cannot explain many flexible or goal-directed actions
              that have been demonstrated behaviorally. We investigate such
              actions by contrasting an RL algorithm that is model based, in
              that it relies on learning a map or model of the task and
              planning within it, to traditional model-free TD learning. To
              distinguish these approaches in humans, we used functional
              magnetic resonance imaging in a continuous spatial navigation
              task, in which frequent changes to the layout of the maze forced
              subjects continually to relearn their favored routes, thereby
              exposing the RL mechanisms used. We sought evidence for the
              neural substrates of such mechanisms by comparing choice behavior
              and blood oxygen level-dependent (BOLD) signals to decision
              variables extracted from simulations of either algorithm. Both
              choices and value-related BOLD signals in striatum, although most
              often associated with TD learning, were better explained by the
              model-based theory. Furthermore, predecessor quantities for the
              model-based value computation were correlated with BOLD signals
              in the medial temporal lobe and frontal cortex. These results
              point to a significant extension of both the computational and
              anatomical substrates for RL in the brain.",
  journal  = "J. Neurosci.",
  volume   =  31,
  number   =  14,
  pages    = "5526--5539",
  year     =  2011,
  language = "en"
}

@ARTICLE{Hampton2006-cx,
  title    = "The role of the ventromedial prefrontal cortex in abstract
              state-based inference during decision making in humans",
  author   = "Hampton, Alan N and Bossaerts, Peter and O'Doherty, John P",
  abstract = "Many real-life decision-making problems incorporate higher-order
              structure, involving interdependencies between different stimuli,
              actions, and subsequent rewards. It is not known whether brain
              regions implicated in decision making, such as the ventromedial
              prefrontal cortex (vmPFC), use a stored model of the task
              structure to guide choice (model-based decision making) or merely
              learn action or state values without assuming higher-order
              structure as in standard reinforcement learning. To discriminate
              between these possibilities, we scanned human subjects with
              functional magnetic resonance imaging while they performed a
              simple decision-making task with higher-order structure,
              probabilistic reversal learning. We found that neural activity in
              a key decision-making region, the vmPFC, was more consistent with
              a computational model that exploits higher-order structure than
              with simple reinforcement learning. These results suggest that
              brain regions, such as the vmPFC, use an abstract model of task
              structure to guide behavioral choice, computations that may
              underlie the human capacity for complex social interactions and
              abstract strategizing.",
  journal  = "J. Neurosci.",
  volume   =  26,
  number   =  32,
  pages    = "8360--8367",
  month    =  aug,
  year     =  2006,
  language = "en"
}

@ARTICLE{Schultz1997-yz,
  title    = "A neural substrate of prediction and reward",
  author   = "Schultz, W and Dayan, P and Montague, P R",
  abstract = "The capacity to predict future events permits a creature to
              detect, model, and manipulate the causal structure of its
              interactions with its environment. Behavioral experiments suggest
              that learning is driven by changes in the expectations about
              future salient events such as rewards and punishments.
              Physiological work has recently complemented these studies by
              identifying dopaminergic neurons in the primate whose fluctuating
              output apparently signals changes or errors in the predictions of
              future salient and rewarding events. Taken together, these
              findings can be understood through quantitative theories of
              adaptive optimizing control.",
  journal  = "Science",
  volume   =  275,
  number   =  5306,
  pages    = "1593--1599",
  month    =  mar,
  year     =  1997,
  language = "en"
}

@ARTICLE{Zhang2018-me,
  title    = "A neural network model for the orbitofrontal cortex and task
              space acquisition during reinforcement learning",
  author   = "Zhang, Zhewei and Cheng, Zhenbo and Lin, Zhongqiao and Nie,
              Chechang and Yang, Tianming",
  abstract = "Reinforcement learning has been widely used in explaining animal
              behavior. In reinforcement learning, the agent learns the value
              of the states in the task, collectively constituting the task
              state space, and uses the knowledge to choose actions and acquire
              desired outcomes. It has been proposed that the orbitofrontal
              cortex (OFC) encodes the task state space during reinforcement
              learning. However, it is not well understood how the OFC acquires
              and stores task state information. Here, we propose a neural
              network model based on reservoir computing. Reservoir networks
              exhibit heterogeneous and dynamic activity patterns that are
              suitable to encode task states. The information can be extracted
              by a linear readout trained with reinforcement learning. We
              demonstrate how the network acquires and stores task structures.
              The network exhibits reinforcement learning behavior and its
              aspects resemble experimental findings of the OFC. Our study
              provides a theoretical explanation of how the OFC may contribute
              to reinforcement learning and a new approach to understanding the
              neural mechanism underlying reinforcement learning.",
  journal  = "PLoS Comput. Biol.",
  volume   =  14,
  number   =  1,
  pages    = "e1005925",
  year     =  2018,
  language = "en"
}

@ARTICLE{Simons1999-qw,
  title    = "Gorillas in our midst: sustained inattentional blindness for
              dynamic events",
  author   = "Simons, D J and Chabris, C F",
  abstract = "With each eye fixation, we experience a richly detailed visual
              world. Yet recent work on visual integration and change direction
              reveals that we are surprisingly unaware of the details of our
              environment from one view to the next: we often do not detect
              large changes to objects and scenes ('change blindness').
              Furthermore, without attention, we may not even perceive objects
              ('inattentional blindness'). Taken together, these findings
              suggest that we perceive and remember only those objects and
              details that receive focused attention. In this paper, we briefly
              review and discuss evidence for these cognitive forms of
              'blindness'. We then present a new study that builds on classic
              studies of divided visual attention to examine inattentional
              blindness for complex objects and events in dynamic scenes. Our
              results suggest that the likelihood of noticing an unexpected
              object depends on the similarity of that object to other objects
              in the display and on how difficult the priming monitoring task
              is. Interestingly, spatial proximity of the critical unattended
              object to attended locations does not appear to affect detection,
              suggesting that observers attend to objects and events, not
              spatial positions. We discuss the implications of these results
              for visual representations and awareness of our visual
              environment.",
  journal  = "Perception",
  volume   =  28,
  number   =  9,
  pages    = "1059--1074",
  year     =  1999,
  language = "en"
}

@ARTICLE{Gronchi2018-yo,
  title    = "Dual Process Theory of Thought and Default Mode Network: A
              Possible Neural Foundation of Fast Thinking",
  author   = "Gronchi, Giorgio and Giovannelli, Fabio",
  journal  = "Front. Psychol.",
  volume   =  9,
  pages    = "1237",
  month    =  jul,
  year     =  2018,
  keywords = "automatic behavior; decision making; default mode network (DMN);
              dual process theory of thought; neural basis of cognition",
  language = "en"
}

@ARTICLE{Bird2008-ht,
  title    = "The hippocampus and memory: insights from spatial processing",
  author   = "Bird, Chris M and Burgess, Neil",
  abstract = "The hippocampus appears to be crucial for long-term episodic
              memory, yet its precise role remains elusive.
              Electrophysiological studies in rodents offer a useful starting
              point for developing models of hippocampal processing in the
              spatial domain. Here we review one such model that points to an
              essential role for the hippocampus in the construction of mental
              images. We explain how this neural-level mechanistic account
              addresses some of the current controversies in the field, such as
              the role of the hippocampus in imagery and short-term memory, and
              discuss its broader implications for the neural bases of episodic
              memory.",
  journal  = "Nat. Rev. Neurosci.",
  volume   =  9,
  number   =  3,
  pages    = "182--194",
  month    =  mar,
  year     =  2008,
  language = "en"
}

@ARTICLE{Clark2013-hr,
  title    = "Whatever next? Predictive brains, situated agents, and the future
              of cognitive science",
  author   = "Clark, Andy",
  abstract = "Brains, it has recently been argued, are essentially prediction
              machines. They are bundles of cells that support perception and
              action by constantly attempting to match incoming sensory inputs
              with top-down expectations or predictions. This is achieved using
              a hierarchical generative model that aims to minimize prediction
              error within a bidirectional cascade of cortical processing. Such
              accounts offer a unifying model of perception and action,
              illuminate the functional role of attention, and may neatly
              capture the special contribution of cortical processing to
              adaptive success. This target article critically examines this
              ``hierarchical prediction machine'' approach, concluding that it
              offers the best clue yet to the shape of a unified science of
              mind and action. Sections 1 and 2 lay out the key elements and
              implications of the approach. Section 3 explores a variety of
              pitfalls and challenges, spanning the evidential, the
              methodological, and the more properly conceptual. The paper ends
              (sections 4 and 5) by asking how such approaches might impact our
              more general vision of mind, experience, and agency.",
  journal  = "Behav. Brain Sci.",
  volume   =  36,
  number   =  3,
  pages    = "181--204",
  year     =  2013,
  language = "en"
}

@ARTICLE{Pfeiffer2013-ef,
  title    = "Hippocampal place-cell sequences depict future paths to
              remembered goals",
  author   = "Pfeiffer, Brad E and Foster, David J",
  abstract = "Effective navigation requires planning extended routes to
              remembered goal locations. Hippocampal place cells have been
              proposed to have a role in navigational planning, but direct
              evidence has been lacking. Here we show that before goal-directed
              navigation in an open arena, the rat hippocampus generates brief
              sequences encoding spatial trajectories strongly biased to
              progress from the subject's current location to a known goal
              location. These sequences predict immediate future behaviour,
              even in cases in which the specific combination of start and goal
              locations is novel. These results indicate that hippocampal
              sequence events characterized previously in linearly constrained
              environments as 'replay' are also capable of supporting a
              goal-directed, trajectory-finding mechanism, which identifies
              important places and relevant behavioural paths, at specific
              times when memory retrieval is required, and in a manner that
              could be used to control subsequent navigational behaviour.",
  journal  = "Nature",
  volume   =  497,
  number   =  7447,
  pages    = "74--79",
  month    =  may,
  year     =  2013,
  language = "en"
}

@ARTICLE{Sutton1991-xk,
  title     = "Dyna, an Integrated Architecture for Learning, Planning, and
               Reacting",
  author    = "Sutton, Richard S",
  journal   = "SIGART Bull.",
  publisher = "ACM",
  volume    =  2,
  number    =  4,
  pages     = "160--163",
  month     =  jul,
  year      =  1991,
  address   = "New York, NY, USA"
}

@ARTICLE{Silver2017-ud,
  title    = "Mastering the game of Go without human knowledge",
  author   = "Silver, David and Schrittwieser, Julian and Simonyan, Karen and
              Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert,
              Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and
              Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre,
              Laurent and van den Driessche, George and Graepel, Thore and
              Hassabis, Demis",
  abstract = "A long-standing goal of artificial intelligence is an algorithm
              that learns, tabula rasa, superhuman proficiency in challenging
              domains. Recently, AlphaGo became the first program to defeat a
              world champion in the game of Go. The tree search in AlphaGo
              evaluated positions and selected moves using deep neural
              networks. These neural networks were trained by supervised
              learning from human expert moves, and by reinforcement learning
              from self-play. Here we introduce an algorithm based solely on
              reinforcement learning, without human data, guidance or domain
              knowledge beyond game rules. AlphaGo becomes its own teacher: a
              neural network is trained to predict AlphaGo's own move
              selections and also the winner of AlphaGo's games. This neural
              network improves the strength of the tree search, resulting in
              higher quality move selection and stronger self-play in the next
              iteration. Starting tabula rasa, our new program AlphaGo Zero
              achieved superhuman performance, winning 100-0 against the
              previously published, champion-defeating AlphaGo.",
  journal  = "Nature",
  volume   =  550,
  number   =  7676,
  pages    = "354--359",
  month    =  oct,
  year     =  2017,
  language = "en"
}

@ARTICLE{Botvinick2019-qf,
  title    = "Reinforcement Learning, Fast and Slow",
  author   = "Botvinick, Matthew and Ritter, Sam and Wang, Jane X and
              Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis",
  abstract = "Deep reinforcement learning (RL) methods have driven impressive
              advances in artificial intelligence in recent years, exceeding
              human performance in domains ranging from Atari to Go to no-limit
              poker. This progress has drawn the attention of cognitive
              scientists interested in understanding human learning. However,
              the concern has been raised that deep RL may be too
              sample-inefficient - that is, it may simply be too slow - to
              provide a plausible model of how humans learn. In the present
              review, we counter this critique by describing recently developed
              techniques that allow deep RL to operate more nimbly, solving
              problems much more quickly than previous methods. Although these
              techniques were developed in an AI context, we propose that they
              may have rich implications for psychology and neuroscience. A key
              insight, arising from these AI methods, concerns the fundamental
              connection between fast RL and slower, more incremental forms of
              learning.",
  journal  = "Trends Cogn. Sci.",
  volume   =  23,
  number   =  5,
  pages    = "408--422",
  year     =  2019,
  language = "en"
}

@ARTICLE{Tsutsui2016-iq,
  title    = "A dynamic code for economic object valuation in prefrontal cortex
              neurons",
  author   = "Tsutsui, Ken-Ichiro and Grabenhorst, Fabian and Kobayashi,
              Shunsuke and Schultz, Wolfram",
  abstract = "Neuronal reward valuations provide the physiological basis for
              economic behaviour. Yet, how such valuations are converted to
              economic decisions remains unclear. Here we show that the
              dorsolateral prefrontal cortex (DLPFC) implements a flexible
              value code based on object-specific valuations by single neurons.
              As monkeys perform a reward-based foraging task, individual DLPFC
              neurons signal the value of specific choice objects derived from
              recent experience. These neuronal object values satisfy
              principles of competitive choice mechanisms, track performance
              fluctuations and follow predictions of a classical behavioural
              model (Herrnstein's matching law). Individual neurons dynamically
              encode both, the updating of object values from recently
              experienced rewards, and their subsequent conversion to object
              choices during decision-making. Decoding from unselected
              populations enables a read-out of motivational and decision
              variables not emphasized by individual neurons. These findings
              suggest a dynamic single-neuron and population value code in
              DLPFC that advances from reward experiences to economic object
              values and future choices.",
  journal  = "Nat. Commun.",
  volume   =  7,
  pages    = "12554",
  year     =  2016,
  language = "en"
}

@ARTICLE{Collins2012-kd,
  title    = "How much of reinforcement learning is working memory, not
              reinforcement learning? A behavioral, computational, and
              neurogenetic analysis: Working memory in reinforcement learning",
  author   = "Collins, Anne G E and Frank, Michael J",
  abstract = "Abstract Instrumental learning involves corticostriatal circuitry
              and the dopaminergic system. This system is typically modeled in
              the reinforcement learning (RL) framework by incrementally
              accumulating reward values of states and actions. However, human
              learning also implicates prefrontal cortical mechanisms involved
              in higher level cognitive functions. The interaction of these
              systems remains poorly understood, and models of human behavior
              often ignore working memory (WM) and therefore incorrectly assign
              behavioral variance to the RL system. Here we designed a task
              that highlights the profound entanglement of these two processes,
              even in simple learning problems. By systematically varying the
              size of the learning problem and delay between stimulus
              repetitions, we separately extracted WM-specific effects of load
              and delay on learning. We propose a new computational model that
              accounts for the dynamic integration of RL and WM processes
              observed in subjects? behavior. Incorporating capacity-limited WM
              into the model allowed us to capture behavioral variance that
              could not be captured in a pure RL framework even if we
              (implausibly) allowed separate RL systems for each set size. The
              WM component also allowed for a more reasonable estimation of a
              single RL process. Finally, we report effects of two genetic
              polymorphisms having relative specificity for prefrontal and
              basal ganglia functions. Whereas the COMT gene coding for
              catechol-O-methyl transferase selectively influenced model
              estimates of WM capacity, the GPR6 gene coding for
              G-protein-coupled receptor 6 influenced the RL learning rate.
              Thus, this study allowed us to specify distinct influences of the
              high-level and low-level cognitive functions on instrumental
              learning, beyond the possibilities offered by simple RL models.",
  journal  = "Eur. J. Neurosci.",
  volume   =  35,
  number   =  7,
  pages    = "1024--1035",
  year     =  2012
}

@ARTICLE{Starkweather2017-yj,
  title    = "Dopamine reward prediction errors reflect hidden-state inference
              across time",
  author   = "Starkweather, Clara Kwon and Babayan, Benedicte M and Uchida,
              Naoshige and Gershman, Samuel J",
  abstract = "Midbrain dopamine neurons signal reward prediction error (RPE),
              or actual minus expected reward. The temporal difference (TD)
              learning model has been a cornerstone in understanding how
              dopamine RPEs could drive associative learning. Classically, TD
              learning imparts value to features that serially track elapsed
              time relative to observable stimuli. In the real world, however,
              sensory stimuli provide ambiguous information about the hidden
              state of the environment, leading to the proposal that TD
              learning might instead compute a value signal based on an
              inferred distribution of hidden states (a 'belief state'). Here
              we asked whether dopaminergic signaling supports a TD learning
              framework that operates over hidden states. We found that
              dopamine signaling showed a notable difference between two tasks
              that differed only with respect to whether reward was delivered
              in a deterministic manner. Our results favor an associative
              learning rule that combines cached values with hidden-state
              inference.",
  journal  = "Nat. Neurosci.",
  volume   =  20,
  number   =  4,
  pages    = "581--589",
  year     =  2017,
  language = "en"
}

@ARTICLE{Friston2009-aa,
  title    = "Reinforcement learning or active inference?",
  author   = "Friston, Karl J and Daunizeau, Jean and Kiebel, Stefan J",
  abstract = "This paper questions the need for reinforcement learning or
              control theory when optimising behaviour. We show that it is
              fairly simple to teach an agent complicated and adaptive
              behaviours using a free-energy formulation of perception. In this
              formulation, agents adjust their internal states and sampling of
              the environment to minimize their free-energy. Such agents learn
              causal structure in the environment and sample it in an adaptive
              and self-supervised fashion. This results in behavioural policies
              that reproduce those optimised by reinforcement learning and
              dynamic programming. Critically, we do not need to invoke the
              notion of reward, value or utility. We illustrate these points by
              solving a benchmark problem in dynamic programming; namely the
              mountain-car problem, using active perception or inference under
              the free-energy principle. The ensuing proof-of-concept may be
              important because the free-energy formulation furnishes a unified
              account of both action and perception and may speak to a
              reappraisal of the role of dopamine in the brain.",
  journal  = "PLoS One",
  volume   =  4,
  number   =  7,
  pages    = "e6421",
  month    =  jul,
  year     =  2009,
  language = "en"
}

@ARTICLE{Silver2016-ir,
  title    = "Mastering the game of Go with deep neural networks and tree
              search",
  author   = "Silver, David and Huang, Aja and Maddison, Chris J and Guez,
              Arthur and Sifre, Laurent and van den Driessche, George and
              Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam,
              Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik
              and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and
              Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray
              and Graepel, Thore and Hassabis, Demis",
  abstract = "The game of Go has long been viewed as the most challenging of
              classic games for artificial intelligence owing to its enormous
              search space and the difficulty of evaluating board positions and
              moves. Here we introduce a new approach to computer Go that uses
              'value networks' to evaluate board positions and 'policy
              networks' to select moves. These deep neural networks are trained
              by a novel combination of supervised learning from human expert
              games, and reinforcement learning from games of self-play.
              Without any lookahead search, the neural networks play Go at the
              level of state-of-the-art Monte Carlo tree search programs that
              simulate thousands of random games of self-play. We also
              introduce a new search algorithm that combines Monte Carlo
              simulation with value and policy networks. Using this search
              algorithm, our program AlphaGo achieved a 99.8\% winning rate
              against other Go programs, and defeated the human European Go
              champion by 5 games to 0. This is the first time that a computer
              program has defeated a human professional player in the
              full-sized game of Go, a feat previously thought to be at least a
              decade away.",
  journal  = "Nature",
  volume   =  529,
  number   =  7587,
  pages    = "484--489",
  month    =  jan,
  year     =  2016,
  language = "en"
}

@ARTICLE{Barto2003-jo,
  title     = "Recent Advances in Hierarchical Reinforcement Learning",
  author    = "Barto, Andrew G and Mahadevan, Sridhar",
  abstract  = "Reinforcement learning is bedeviled by the curse of
               dimensionality: the number of parameters to be learned grows
               exponentially with the size of any compact encoding of a state.
               Recent attempts to combat the curse of dimensionality have
               turned to principled ways of exploiting temporal abstraction,
               where decisions are not required at each step, but rather invoke
               the execution of temporally-extended activities which follow
               their own policies until termination. This leads naturally to
               hierarchical control architectures and associated learning
               algorithms. We review several approaches to temporal abstraction
               and hierarchical organization that machine learning researchers
               have recently developed. Common to these approaches is a
               reliance on the theory of semi-Markov decision processes, which
               we emphasize in our review. We then discuss extensions of these
               ideas to concurrent activities, multiagent coordination, and
               hierarchical memory for addressing partial observability.
               Concluding remarks address open challenges facing the further
               development of reinforcement learning in a hierarchical setting.",
  journal   = "Discrete Event Dyn. Syst.: Theory Appl.",
  publisher = "Springer",
  volume    =  13,
  number    =  1,
  pages     = "41--77",
  month     =  jan,
  year      =  2003
}

@ARTICLE{Wang2018-ij,
  title    = "Prefrontal cortex as a meta-reinforcement learning system",
  author   = "Wang, Jane X and Kurth-Nelson, Zeb and Kumaran, Dharshan and
              Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and
              Hassabis, Demis and Botvinick, Matthew",
  abstract = "Over the past 20 years, neuroscience research on reward-based
              learning has converged on a canonical model, under which the
              neurotransmitter dopamine 'stamps in' associations between
              situations, actions and rewards by modulating the strength of
              synaptic connections between neurons. However, a growing number
              of recent findings have placed this standard model under strain.
              We now draw on recent advances in artificial intelligence to
              introduce a new theory of reward-based learning. Here, the
              dopamine system trains another part of the brain, the prefrontal
              cortex, to operate as its own free-standing learning system. This
              new perspective accommodates the findings that motivated the
              standard model, but also deals gracefully with a wider range of
              observations, providing a fresh foundation for future research.",
  journal  = "Nat. Neurosci.",
  year     =  2018,
  language = "en"
}

@ARTICLE{Wilson2014-dq,
  title    = "Orbitofrontal cortex as a cognitive map of task space",
  author   = "Wilson, Robert C and Takahashi, Yuji K and Schoenbaum, G and Niv,
              Yael",
  abstract = "Orbitofrontal cortex (OFC) has long been known to play an
              important role in decision making. However, the exact nature of
              that role has remained elusive. Here, we propose a unifying
              theory of OFC function. We hypothesize that OFC provides an
              abstraction of currently available information in the form of a
              labeling of the current task state, which is used for
              reinforcement learning (RL) elsewhere in the brain. This function
              is especially critical when task states include unobservable
              information, for instance, from working memory. We use this
              framework to explain classic findings in reversal learning,
              delayed alternation, extinction, and devaluation as well as more
              recent findings showing the effect of OFC lesions on the firing
              of dopaminergic neurons in ventral tegmental area (VTA) in
              rodents performing an RL task. In addition, we generate a number
              of testable experimental predictions that can distinguish our
              theory from other accounts of OFC function.",
  journal  = "Neuron",
  volume   =  81,
  number   =  2,
  pages    = "267--279",
  month    =  jan,
  year     =  2014,
  language = "en"
}

@ARTICLE{Thorndike1898-ef,
  title     = "Animal intelligence: an experimental study of the associative
               processes in animals",
  author    = "Thorndike, Edward L",
  abstract  = "Abstract This monograph is an attempt at an explanation of the
               nature of the process of association in the animal mind. It
               discusses experiments with cats, dogs, and chicks. Imitation in
               these animals is discussed as well. Reasoning, inference, and
               associative processes are also explored.(PsycINFO Database
               Record (c) 2016 APA, all rights reserved)",
  journal   = "The Psychological Review: Monograph Supplements",
  publisher = "The Macmillan Company",
  volume    =  2,
  number    =  4,
  pages     = "i",
  year      =  1898
}

@ARTICLE{Silver2018-gk,
  title    = "A general reinforcement learning algorithm that masters chess,
              shogi, and Go through self-play",
  author   = "Silver, David and Hubert, Thomas and Schrittwieser, Julian and
              Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and
              Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and
              Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and
              Hassabis, Demis",
  abstract = "The game of chess is the longest-studied domain in the history of
              artificial intelligence. The strongest programs are based on a
              combination of sophisticated search techniques, domain-specific
              adaptations, and handcrafted evaluation functions that have been
              refined by human experts over several decades. By contrast, the
              AlphaGo Zero program recently achieved superhuman performance in
              the game of Go by reinforcement learning from self-play. In this
              paper, we generalize this approach into a single AlphaZero
              algorithm that can achieve superhuman performance in many
              challenging games. Starting from random play and given no domain
              knowledge except the game rules, AlphaZero convincingly defeated
              a world champion program in the games of chess and shogi
              (Japanese chess), as well as Go.",
  journal  = "Science",
  volume   =  362,
  number   =  6419,
  pages    = "1140--1144",
  year     =  2018,
  language = "en"
}

@ARTICLE{Harlow1949-is,
  title    = "The formation of learning sets",
  author   = "Harlow, H F",
  journal  = "Psychol. Rev.",
  volume   =  56,
  number   =  1,
  pages    = "51--65",
  month    =  jan,
  year     =  1949,
  keywords = "LEARNING",
  language = "en"
}

@MISC{Vinyals2019-bk,
  title        = "{{AlphaStar}: Mastering the {Real-Time} Strategy Game
                  {StarCraft} {II}}",
  author       = "Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and
                  Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojtek and
                  Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell,
                  Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel
                  and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and
                  Dalibard, Valentin and Choi, David and Sifre, Laurent and
                  Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai,
                  Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar
                  and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and
                  Yogatama, Dani and Cohen, Julia and McKinney, Katrina and
                  Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and
                  Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and
                  Silver, David",
  year         =  2019,
  howpublished = "\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}"
}

@ARTICLE{Armus2006-jf,
  title     = "Discrimination learning and extinction in paramecia (P.
               caudatum)",
  author    = "Armus, Harvard L and Montgomery, Amber R and Gurney, Rebecca L",
  abstract  = "Prior attempts to condition a one-celled organism, paramecium,
               by either classical or instrumental procedures have yielded both
               positive and negative results. As the results of those studies
               may be subject to several interpretations other than one
               indicating learning, it was decided to use a more traditional
               technique for the present study. This experiment was designed to
               assess whether aversive electric shock could be used to train
               paramecia on a brightness discrimination task, a procedure that
               has been used in animal learning research. The results indicated
               that such learning may have occurred.",
  journal   = "Psychol. Rep.",
  publisher = "journals.sagepub.com",
  volume    =  98,
  number    =  3,
  pages     = "705--711",
  month     =  jun,
  year      =  2006,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wise2004-rs,
  title     = "Dopamine, learning and motivation",
  author    = "Wise, Roy A",
  abstract  = "The hypothesis that dopamine is important for reward has been
               proposed in a number of forms, each of which has been
               challenged. Normally, rewarding stimuli such as food, water,
               lateral hypothalamic brain stimulation and several drugs of
               abuse become ineffective as rewards in animals given
               performance-sparing doses of dopamine antagonists. Dopamine
               release in the nucleus accumbens has been linked to the efficacy
               of these unconditioned rewards, but dopamine release in a
               broader range of structures is implicated in the'stamping …",
  journal   = "Nat. Rev. Neurosci.",
  publisher = "nature.com",
  volume    =  5,
  number    =  6,
  pages     = "483--494",
  month     =  jun,
  year      =  2004,
  language  = "en"
}

@INPROCEEDINGS{Bengio1991-mx,
  title     = "Learning a synaptic learning rule",
  booktitle = "{IJCNN-91-Seattle} International Joint Conference on Neural
               Networks",
  author    = "Bengio, Y and Bengio, S and Cloutier, J",
  abstract  = "Summary form only given, as follows. The authors discuss an
               original approach to neural modeling based on the idea of
               searching, with learning methods, for a synaptic learning rule
               which is biologically plausible and yields networks that are
               able to learn to perform difficult tasks. The proposed method of
               automatically finding the learning rule relies on the idea of
               considering the synaptic modification rule as a parametric
               function. This function has local inputs and is the same in many
               neurons. The parameters that define this function can be
               estimated with known learning methods. For this optimization,
               particular attention is given to gradient descent and genetic
               algorithms. In both cases, estimation of this function consists
               of a joint global optimization of the synaptic modification
               function and the networks that are learning to perform some
               tasks. Both network architecture and the learning function can
               be designed within constraints derived from biological
               knowledge.>",
  volume    = "ii",
  pages     = "969 vol.2--",
  month     =  jul,
  year      =  1991,
  keywords  = "learning systems;neural nets;search problems;gradient descent
               algorithms;synaptic learning rule;neural
               modeling;searching;synaptic modification rule;genetic
               algorithms;joint global optimization;Learning systems;Computer
               science;Biological system modeling;Biology;Neurons;Genetic
               algorithms"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Lengyel2008-sy,
  title     = "Hippocampal Contributions to Control: The Third Way",
  booktitle = "Advances in Neural Information Processing Systems 20",
  author    = "Lengyel, M{\'a}t{\'e} and Dayan, Peter",
  editor    = "Platt, J C and Koller, D and Singer, Y and Roweis, S T",
  abstract  = "Recent experimental studies have focused on the specialization
               of different neural structures for different types of
               instrumental behavior. Recent theoretical work has provided
               normative accounts for why there should be more than one control
               system, and how the output of …",
  publisher = "Curran Associates, Inc.",
  pages     = "889--896",
  year      =  2008
}

@MISC{OpenAI2018-km,
  title        = "{AI} and Compute",
  booktitle    = "{OpenAI} Blog",
  author       = "{OpenAI}",
  abstract     = "Since 2012, the amount of compute used in the largest AI
                  training runs has been increasing exponentially with a 3.5
                  month doubling time (by comparison, Moore's Law had an 18
                  month doubling period).",
  publisher    = "OpenAI Blog",
  month        =  may,
  year         =  2018,
  howpublished = "\url{https://blog.openai.com/ai-and-compute/}",
  note         = "Accessed: 2018-10-18"
}

@ARTICLE{Kerns2004-uo,
  title     = "Anterior cingulate conflict monitoring and adjustments in
               control",
  author    = "Kerns, John G and Cohen, Jonathan D and MacDonald, 3rd, Angus W
               and Cho, Raymond Y and Stenger, V Andrew and Carter, Cameron S",
  abstract  = "Conflict monitoring by the anterior cingulate cortex (ACC) has
               been posited to signal a need for greater cognitive control,
               producing neural and behavioral adjustments. However, the very
               occurrence of behavioral adjustments after conflict has been
               questioned, along with suggestions that there is no direct
               evidence of ACC conflict-related activity predicting subsequent
               neural or behavioral adjustments in control. Using the Stroop
               color-naming task and controlling for repetition effects, we
               demonstrate that ACC conflict-related activity predicts both
               greater prefrontal cortex activity and adjustments in behavior,
               supporting a role of ACC conflict monitoring in the engagement
               of cognitive control.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  303,
  number    =  5660,
  pages     = "1023--1026",
  month     =  feb,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Babayan2018-ix,
  title    = "Belief state representation in the dopamine system",
  author   = "Babayan, Benedicte M and Uchida, Naoshige and Gershman, Samuel J",
  abstract = "Learning to predict future outcomes is critical for driving
              appropriate behaviors. Reinforcement learning (RL) models have
              successfully accounted for such learning, relying on reward
              prediction errors (RPEs) signaled by midbrain dopamine neurons.
              It has been proposed that when sensory data provide only
              ambiguous information about which state an animal is in, it can
              predict reward based on a set of probabilities assigned to
              hypothetical states (called the belief state). Here we examine
              how dopamine RPEs and subsequent learning are regulated under
              state uncertainty. Mice are first trained in a task with two
              potential states defined by different reward amounts. During
              testing, intermediate-sized rewards are given in rare trials.
              Dopamine activity is a non-monotonic function of reward size,
              consistent with RL models operating on belief states.
              Furthermore, the magnitude of dopamine responses quantitatively
              predicts changes in behavior. These results establish the
              critical role of state inference in RL.",
  journal  = "Nat. Commun.",
  volume   =  9,
  number   =  1,
  pages    = "1891",
  year     =  2018,
  language = "en"
}

@UNPUBLISHED{Schuck2018-hh,
  title    = "Sequential replay of non-spatial task states in the human
              hippocampus",
  author   = "Schuck, Nicolas W and Niv, Yael",
  abstract = "Neurophysiological research has found that previously experienced
              sequences of spatial events are reactivated in the hippocampus of
              rodents during wakeful rest. This phenomenon has become a
              cornerstone of modern theories of memory and decision making.
              Yet, whether hippocampal sequence reactivation at rest is of
              general importance also for humans and non-spatial tasks has
              remained unclear. Here, we investigated sequences of fMRI BOLD
              activation patterns in humans during wakeful rest following a
              sequential non-spatial decision-making task. We found that
              pattern reactivations within the human hippocampus reflected the
              order of previous task state sequences, and that the extent of
              this offline reactivation was related to the on-task
              representation of task states in the orbitofrontal cortex.
              Permutation analyses and fMRI signal simulations confirmed that
              these results reflected underlying BOLD activity, and showed that
              our novel statistical analyses are, in principle, sensitive to
              sequential neural events occurring as fast as one hundred
              milliseconds apart. Our results support the importance of
              sequential reactivation in the human hippocampus for decision
              making, and establish the feasibility of investigating such rapid
              signals with fMRI, despite its substantial temporal limitations.",
  journal  = "bioRxiv",
  pages    = "315978",
  year     =  2018,
  language = "en"
}

@ARTICLE{Gabbert2003-ms,
  title    = "Memory conformity: can eyewitnesses influence each other's
              memories for an event?",
  author   = "Gabbert, Fiona and Memon, Amina and Allan, Kevin",
  abstract = "Abstract The current study investigated memory conformity effects
              between individuals who witness and then discuss a criminal
              event, employing a novel procedure whereby each member of a dyad
              watches a different video of the same event. Each video contained
              unique items that were thus seen only by one witness. Dyads in
              one condition were encouraged to discuss the event before each
              witness (individually) performed a recall test, while in a
              control condition dyads were not allowed to discuss the event
              prior to recall. A significant proportion (71\%) of witnesses who
              had discussed the event went on to mistakenly recall items
              acquired during the discussion. There were no age-related
              differences in susceptibility to these memory conformity effects
              in younger (18?30 years) as compared to older (60?80 years)
              participants. Possible social and cognitive mechanisms underlying
              the distortions of memory due to conformity are discussed.
              Copyright ? 2003 John Wiley \& Sons, Ltd.",
  journal  = "Appl. Cogn. Psychol.",
  volume   =  17,
  number   =  5,
  pages    = "533--543",
  month    =  jul,
  year     =  2003
}

@ARTICLE{OpenAI2019-xbdd,
  title         = "Dota 2 with Large Scale Deep Reinforcement Learning",
  author        = "{OpenAI} and {:} and Berner, Christopher and Brockman, Greg
                   and Chan, Brooke and Cheung, Vicki and D{\k e}biak,
                   Przemys{\l}aw and Dennison, Christy and Farhi, David and
                   Fischer, Quirin and Hashme, Shariq and Hesse, Chris and
                   J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine
                   and Pachocki, Jakub and Petrov, Michael and de Oliveira
                   Pinto, Henrique Pond{\'e} and Raiman, Jonathan and Salimans,
                   Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor,
                   Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip
                   and Zhang, Susan",
  abstract      = "On April 13th, 2019, OpenAI Five became the first AI system
                   to defeat the world champions at an esports game. The game
                   of Dota 2 presents novel challenges for AI systems such as
                   long time horizons, imperfect information, and complex,
                   continuous state-action spaces, all challenges which will
                   become increasingly central to more capable AI systems.
                   OpenAI Five leveraged existing reinforcement learning
                   techniques, scaled to learn from batches of approximately 2
                   million frames every 2 seconds. We developed a distributed
                   training system and tools for continual training which
                   allowed us to train OpenAI Five for 10 months. By defeating
                   the Dota 2 world champion (Team OG), OpenAI Five
                   demonstrates that self-play reinforcement learning can
                   achieve superhuman performance on a difficult task.",
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1912.06680"
}

@BOOK{Minsky1988-bl,
  title     = "Society Of Mind",
  author    = "Minsky, Marvin",
  abstract  = "Marvin Minsky -- one of the fathers of computer science and
               cofounder of the Artificial Intelligence Laboratory at MIT --
               gives a revolutionary answer to the age-old question: ``How does
               the mind work?'' Minsky brilliantly portrays the mind as a
               ``society'' of tiny components that are themselves mindless.
               Mirroring his theory, Minsky boldly casts The Society of Mind as
               an intellectual puzzle whose pieces are assembled along the way.
               Each chapter -- on a self-contained page -- corresponds to a
               piece in the puzzle. As the pages turn, a unified theory of the
               mind emerges, like a mosaic. Ingenious, amusing, and easy to
               read, The Society of Mind is an adventure in imagination.",
  publisher = "Simon and Schuster",
  month     =  mar,
  year      =  1988,
  language  = "en"
}

@MISC{OpenAI2018-ne,
  title        = "{OpenAI} Five",
  booktitle    = "{OpenAI} Blog",
  author       = "{OpenAI}",
  abstract     = "Our team of five neural networks, OpenAI Five, has started to
                  defeat amateur human teams at Dota 2.",
  publisher    = "OpenAI Blog",
  month        =  jun,
  year         =  2018,
  howpublished = "\url{https://blog.openai.com/openai-five/}",
  note         = "Accessed: 2019-2-26"
}

@ARTICLE{Gershman2014-aq,
  title    = "Retrospective revaluation in sequential decision making: a tale
              of two systems",
  author   = "Gershman, Samuel J and Markman, Arthur B and Otto, A Ross",
  abstract = "Recent computational theories of decision making in humans and
              animals have portrayed 2 systems locked in a battle for control
              of behavior. One system--variously termed model-free or
              habitual--favors actions that have previously led to reward,
              whereas a second--called the model-based or goal-directed
              system--favors actions that causally lead to reward according to
              the agent's internal model of the environment. Some evidence
              suggests that control can be shifted between these systems using
              neural or behavioral manipulations, but other evidence suggests
              that the systems are more intertwined than a competitive account
              would imply. In 4 behavioral experiments, using a retrospective
              revaluation design and a cognitive load manipulation, we show
              that human decisions are more consistent with a cooperative
              architecture in which the model-free system controls behavior,
              whereas the model-based system trains the model-free system by
              replaying and simulating experience.",
  journal  = "J. Exp. Psychol. Gen.",
  volume   =  143,
  number   =  1,
  pages    = "182--194",
  month    =  feb,
  language = "en"
}

@ARTICLE{Carter1998-wa,
  title     = "Anterior cingulate cortex, error detection, and the online
               monitoring of performance",
  author    = "Carter, C S and Braver, T S and Barch, D M and Botvinick, M M
               and Noll, D and Cohen, J D",
  abstract  = "An unresolved question in neuroscience and psychology is how the
               brain monitors performance to regulate behavior. It has been
               proposed that the anterior cingulate cortex (ACC), on the medial
               surface of the frontal lobe, contributes to performance
               monitoring by detecting errors. In this study, event-related
               functional magnetic resonance imaging was used to examine ACC
               function. Results confirm that this region shows activity during
               erroneous responses. However, activity was also observed in the
               same region during correct responses under conditions of
               increased response competition. This suggests that the ACC
               detects conditions under which errors are likely to occur rather
               than errors themselves.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  280,
  number    =  5364,
  pages     = "747--749",
  month     =  may,
  year      =  1998,
  language  = "en"
}

@ARTICLE{Gershman2013-bo,
  title     = "Perceptual estimation obeys Occam's razor",
  author    = "Gershman, Samuel J and Niv, Yael",
  abstract  = "Theoretical models of unsupervised category learning postulate
               that humans ``invent'' categories to accommodate new patterns,
               but tend to group stimuli into a small number of categories.
               This ``Occam's razor'' principle is motivated by normative rules
               of statistical inference. If categories influence perception,
               then one should find effects of category invention on simple
               perceptual estimation. In a series of experiments, we tested
               this prediction by asking participants to estimate the number of
               colored circles on a computer screen, with the number of circles
               drawn from a color-specific distribution. When the distributions
               associated with each color overlapped substantially,
               participants' estimates were biased toward values intermediate
               between the two means, indicating that subjects ignored the
               color of the circles and grouped different-colored stimuli into
               one perceptual category. These data suggest that humans favor
               simpler explanations of sensory inputs. In contrast, when the
               distributions associated with each color overlapped minimally,
               the bias was reduced (i.e., the estimates for each color were
               closer to the true means), indicating that sensory evidence for
               more complex explanations can override the simplicity bias. We
               present a rational analysis of our task, showing how these
               qualitative patterns can arise from Bayesian computations.",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  4,
  pages     = "623",
  year      = 2013,
  keywords  = "Bayesian inference; categorization; perception; simplicity;
               unsupervised learning",
  language  = "en"
}

@ARTICLE{Kuhn2016-vb,
  title    = "The Vanishing Ball Illusion: A new perspective on the perception
              of dynamic events",
  author   = "Kuhn, Gustav and Rensink, Ronald A",
  abstract = "Our perceptual experience is largely based on prediction, and as
              such can be influenced by knowledge of forthcoming events. This
              susceptibility is commonly exploited by magicians. In the
              Vanishing Ball Illusion, for example, a magician tosses a ball in
              the air a few times and then pretends to throw the ball again,
              whilst secretly concealing it in his hand. Most people claim to
              see the ball moving upwards and then vanishing, even though it
              did not leave the magician's hand (Kuhn \& Land, 2006; Triplett,
              1900). But what exactly can such illusions tell us? We
              investigated here whether seeing a real action before the pretend
              one was necessary for the Vanishing Ball Illusion. Participants
              either saw a real action immediately before the fake one, or only
              a fake action. Nearly one third of participants experienced the
              illusion with the fake action alone, while seeing the real action
              beforehand enhanced this effect even further. Our results
              therefore suggest that perceptual experience relies both on
              long-term knowledge of what an action should look like, as well
              as exemplars from the immediate past. In addition, whilst there
              was a forward displacement of perceived location in perceptual
              experience, this was not found for oculomotor responses,
              consistent with the proposal that two separate systems are
              involved in visual perception.",
  journal  = "Cognition",
  volume   =  148,
  pages    = "64--70",
  month    =  mar,
  year     =  2016,
  keywords = "Illusion; Magic; Perceptual experience; Prediction; Priming; Two
              visual system hypothesis; Visual attention",
  language = "en"
}

@ARTICLE{Schmidhuber1987-nd,
  title   = "Evolutionary principles in self-referential learning",
  author  = "Schmidhuber, Jurgen",
  journal = "On learning how to learn: The meta-meta-. . . hook. ) Diploma
             thesis, Institut f. Informatik, Tech. Univ. Munich",
  year    =  1987
}

@ARTICLE{Goodale1992-wa,
  title    = "Separate visual pathways for perception and action",
  author   = "Goodale, M A and Milner, A D",
  abstract = "Accumulating neuropsychological, electrophysiological and
              behavioural evidence suggests that the neural substrates of
              visual perception may be quite distinct from those underlying the
              visual control of actions. In other words, the set of object
              descriptions that permit identification and recognition may be
              computed independently of the set of descriptions that allow an
              observer to shape the hand appropriately to pick up an object. We
              propose that the ventral stream of projections from the striate
              cortex to the inferotemporal cortex plays the major role in the
              perceptual identification of objects, while the dorsal stream
              projecting from the striate cortex to the posterior parietal
              region mediates the required sensorimotor transformations for
              visually guided actions directed at such objects.",
  journal  = "Trends Neurosci.",
  volume   =  15,
  number   =  1,
  pages    = "20--25",
  month    =  jan,
  year     =  1992,
  language = "en"
}

@ARTICLE{Stamatakis2012-mp,
  title     = "Activation of lateral habenula inputs to the ventral midbrain
               promotes behavioral avoidance",
  author    = "Stamatakis, Alice M and Stuber, Garret D",
  abstract  = "Lateral habenula (LHb) projections to the ventral midbrain,
               including the rostromedial tegmental nucleus (RMTg), convey
               negative reward-related information, but the behavioral
               ramifications of selective activation of this pathway remain
               unexplored. We found that exposure to aversive stimuli in mice
               increased LHb excitatory drive onto RMTg neurons. Furthermore,
               optogenetic activation of this pathway promoted active, passive
               and conditioned behavioral avoidance. Thus, activity of LHb
               efferents to the midbrain is aversive but can also serve to
               negatively reinforce behavioral responding.",
  journal   = "Nat. Neurosci.",
  publisher = "nature.com",
  volume    =  15,
  number    =  8,
  pages     = "1105--1107",
  month     =  jun,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Intraub1989-za,
  title    = "Wide-angle memories of close-up scenes",
  author   = "Intraub, H and Richardson, M",
  abstract = "We report a picture-memory phenomenon in which subjects' recall
              and recognition of photographed scenes reveal a pronounced
              extension of the pictures' boundaries. After viewing 20 pictures
              for 15 s each, 37 undergraduates exhibited this striking
              distortion; 95\% of their drawings included information that had
              not been physically present but that would have been likely to
              have existed just outside the camera's field of view (Experiment
              1). To determine if boundary extension is limited to recall and
              drawing ability, Experiment 2 tested recognition memory for
              boundaries. Eighty-five undergraduates rated targets and
              distractors on a boundary-placement scale. Subjects rated target
              pictures as being closer up than before and frequently mistook
              extended-boundary distractors as targets. Results are discussed
              in terms of picture comprehension and memory. In addition to its
              theoretical value, discovery of the phenomenon demonstrates the
              importance of more widespread use of open-ended tests in
              picture-memory methodology.",
  journal  = "J. Exp. Psychol. Learn. Mem. Cogn.",
  volume   =  15,
  number   =  2,
  pages    = "179--187",
  month    =  mar,
  year     =  1989,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rescorla1972-na,
  title     = "A theory of Pavlovian conditioning: Variations in the
               effectiveness of reinforcement and nonreinforcement",
  author    = "Rescorla, Robert A and Wagner, Allan R",
  abstract  = "BACKGROUND The background data pauern embraces a considerable
               range of phenomena. At the core, however, is a rather simple set
               of observations involving Pavlovian conditioning with compound
               CSs. Suppose we have inferential knowledge concerning the`` …",
  journal   = "Classical conditioning II: Current research and theory",
  publisher = "New-York",
  volume    =  2,
  pages     = "64--99",
  year      =  1972
}

@ARTICLE{Schwartenbeck2015-pf,
  title    = "The Dopaminergic Midbrain Encodes the Expected Certainty about
              Desired Outcomes",
  author   = "Schwartenbeck, Philipp and FitzGerald, Thomas H B and Mathys,
              Christoph and Dolan, Ray and Friston, Karl",
  abstract = "Dopamine plays a key role in learning; however, its exact
              function in decision making and choice remains unclear. Recently,
              we proposed a generic model based on active (Bayesian) inference
              wherein dopamine encodes the precision of beliefs about optimal
              policies. Put simply, dopamine discharges reflect the confidence
              that a chosen policy will lead to desired outcomes. We designed a
              novel task to test this hypothesis, where subjects played a
              ``limited offer'' game in a functional magnetic resonance imaging
              experiment. Subjects had to decide how long to wait for a high
              offer before accepting a low offer, with the risk of losing
              everything if they waited too long. Bayesian model comparison
              showed that behavior strongly supported active inference, based
              on surprise minimization, over classical utility maximization
              schemes. Furthermore, midbrain activity, encompassing dopamine
              projection neurons, was accurately predicted by trial-by-trial
              variations in model-based estimates of precision. Our findings
              demonstrate that human subjects infer both optimal policies and
              the precision of those inferences, and thus support the notion
              that humans perform hierarchical probabilistic Bayesian
              inference. In other words, subjects have to infer both what they
              should do as well as how confident they are in their choices,
              where confidence may be encoded by dopaminergic firing.",
  journal  = "Cereb. Cortex",
  volume   =  25,
  number   =  10,
  pages    = "3434--3445",
  month    =  oct,
  year     =  2015,
  keywords = "active inference; confidence; dopamine; neuroeconomics; precision",
  language = "en"
}

@BOOK{James1890-jl,
  title     = "The Principles of Psychology -",
  author    = "James, William",
  abstract  = "This is Part I of a fascinating insight into the history of
               psychological theory by American psychologist William James.
               ``The Principles of Psychology'' will appeal to those with an
               interest in the history and development of modern psychology and
               related techniques. Contents Include: ``The Scope of
               Psychology``, ''The Functions of the Brain``, ''On Some General
               Conditions of Brain-Activity``, ''Habit``, ''The
               Automaton-Theory``, ''The Mind-stuff Theory``, ''The Methods and
               Snares of Psychology``, ''The Relations of Minds to Other
               Things``, ''The Stream of Thought``, ''The Consciousness of
               Self'', etc. Many vintage books such as this are increasingly
               scarce and expensive. It is with this in mind that we are
               republishing this volume now in an affordable, modern,
               high-quality edition complete with a specially-commissioned new
               biography of the author.",
  publisher = "Henry Holt and Company",
  year      =  1890,
  language  = "en"
}

@ARTICLE{Joel2002-rh,
  title    = "Actor--critic models of the basal ganglia: new anatomical and
              computational perspectives",
  author   = "Joel, Daphna and Niv, Yael and Ruppin, Eytan",
  abstract = "A large number of computational models of information processing
              in the basal ganglia have been developed in recent years.
              Prominent in these are actor--critic models of basal ganglia
              functioning, which build on the strong resemblance between
              dopamine neuron activity and the temporal difference prediction
              error signal in the critic, and between dopamine-dependent
              long-term synaptic plasticity in the striatum and learning guided
              by a prediction error signal in the actor. We selectively review
              several actor--critic models of the basal ganglia with an
              emphasis on two important aspects: the way in which models of the
              critic reproduce the temporal dynamics of dopamine firing, and
              the extent to which models of the actor take into account known
              basal ganglia anatomy and physiology. To complement the efforts
              to relate basal ganglia mechanisms to reinforcement learning
              (RL), we introduce an alternative approach to modeling a critic
              network, which uses Evolutionary Computation techniques to
              `evolve' an optimal RL mechanism, and relate the evolved
              mechanism to the basic model of the critic. We conclude our
              discussion of models of the critic by a critical discussion of
              the anatomical plausibility of implementations of a critic in
              basal ganglia circuitry, and conclude that such implementations
              build on assumptions that are inconsistent with the known anatomy
              of the basal ganglia. We return to the actor component of the
              actor--critic model, which is usually modeled at the striatal
              level with very little detail. We describe an alternative model
              of the basal ganglia which takes into account several important,
              and previously neglected, anatomical and physiological
              characteristics of basal ganglia--thalamocortical connectivity
              and suggests that the basal ganglia performs reinforcement-biased
              dimensionality reduction of cortical inputs. We further suggest
              that since such selective encoding may bias the representation at
              the level of the frontal cortex towards the selection of rewarded
              plans and actions, the reinforcement-driven dimensionality
              reduction framework may serve as a basis for basal ganglia actor
              models. We conclude with a short discussion of the dual role of
              the dopamine signal in RL and in behavioral switching.",
  journal  = "Neural Netw.",
  volume   =  15,
  number   =  4,
  pages    = "535--547",
  month    =  jun,
  year     =  2002,
  keywords = "Basal ganglia; Dopamine; Reinforcement learning; Actor--critic;
              Dimensionality reduction; Evolutionary computation; Behavioral
              switching; Striosomes/patches"
}

@ARTICLE{Miller2017-su,
  title    = "Dorsal hippocampus contributes to model-based planning",
  author   = "Miller, Kevin J and Botvinick, Matthew M and Brody, Carlos D",
  abstract = "Planning can be defined as action selection that leverages an
              internal model of the outcomes likely to follow each possible
              action. Its neural mechanisms remain poorly understood. Here we
              adapt recent advances from human research for rats, presenting
              for the first time an animal task that produces many trials of
              planned behavior per session, making multitrial rodent
              experimental tools available to study planning. We use part of
              this toolkit to address a perennially controversial issue in
              planning: the role of the dorsal hippocampus. Although
              prospective hippocampal representations have been proposed to
              support planning, intact planning in animals with damaged
              hippocampi has been repeatedly observed. Combining formal
              algorithmic behavioral analysis with muscimol inactivation, we
              provide causal evidence directly linking dorsal hippocampus with
              planning behavior. Our results and methods open the door to new
              and more detailed investigations of the neural mechanisms of
              planning in the hippocampus and throughout the brain.",
  journal  = "Nat. Neurosci.",
  volume   =  20,
  number   =  9,
  pages    = "1269--1276",
  month    =  sep,
  year     =  2017,
  language = "en"
}

@ARTICLE{Gardner2018-vj,
  title    = "Rethinking dopamine as generalized prediction error",
  author   = "Gardner, Matthew P H and Schoenbaum, Geoffrey and Gershman,
              Samuel J",
  abstract = "Midbrain dopamine neurons are commonly thought to report a reward
              prediction error (RPE), as hypothesized by reinforcement learning
              (RL) theory. While this theory has been highly successful,
              several lines of evidence suggest that dopamine activity also
              encodes sensory prediction errors unrelated to reward. Here, we
              develop a new theory of dopamine function that embraces a broader
              conceptualization of prediction errors. By signalling errors in
              both sensory and reward predictions, dopamine supports a form of
              RL that lies between model-based and model-free algorithms. This
              account remains consistent with current canon regarding the
              correspondence between dopamine transients and RPEs, while also
              accounting for new data suggesting a role for these signals in
              phenomena such as sensory preconditioning and identity
              unblocking, which ostensibly draw upon knowledge beyond reward
              predictions.",
  journal  = "Proc. Biol. Sci.",
  volume   =  285,
  number   =  1891,
  year     =  2018,
  keywords = "reinforcement learning; successor representation; temporal
              difference learning",
  language = "en"
}

@MISC{The_AlphaStar_team_undated-yj,
  title        = "{AlphaStar}: Mastering the {Real-Time} Strategy Game
                  {StarCraft} {II}",
  booktitle    = "Deepmind",
  author       = "{The AlphaStar team}",
  abstract     = "Games have been used for decades as an important way to test
                  and evaluate the performance of artificial intelligence
                  systems. As capabilities have increased, the research
                  community has sought games with increasing complexity that
                  capture different elements of intelligence required to solve
                  scientific and real-world problems. In recent years,
                  StarCraft, considered to be one of the most challenging
                  Real-Time Strategy (RTS) games and one of the longest-played
                  esports of all time, has emerged by consensus as a ``grand
                  challenge'' for AI research.",
  howpublished = "\url{https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii}",
  note         = "Accessed: 2020-1-29"
}

@ARTICLE{Gurney2001-lm,
  title     = "A computational model of action selection in the basal ganglia.
               I. A new functional anatomy",
  author    = "Gurney, K and Prescott, T J and Redgrave, P",
  abstract  = "We present a biologically plausible model of processing
               intrinsic to the basal ganglia based on the computational
               premise that action selection is a primary role of these central
               brain structures. By encoding the propensity for selecting a
               given action in a scalar value (the salience), it is shown that
               action selection may be recast in terms of signal selection. The
               generic properties of signal selection are defined and neural
               networks for this type of computation examined. A comparison
               between these networks and basal ganglia anatomy leads to a
               novel functional decomposition of the basal ganglia architecture
               into 'selection' and 'control' pathways. The former pathway
               performs the selection per se via a feedforward off-centre
               on-surround network. The control pathway regulates the action of
               the selection pathway to ensure its effective operation, and
               synergistically complements its dopaminergic modulation. The
               model contrasts with the prevailing functional segregation of
               basal ganglia into 'direct' and 'indirect' pathways.",
  journal   = "Biol. Cybern.",
  publisher = "Springer",
  volume    =  84,
  number    =  6,
  pages     = "401--410",
  month     =  jun,
  year      =  2001,
  language  = "en"
}

@INCOLLECTION{Tokic2011-cd,
  title     = "{Value-Difference} Based Exploration: Adaptive Control between
               {Epsilon-Greedy} and Softmax",
  booktitle = "{KI} 2011: Advances in Artificial Intelligence",
  author    = "Tokic, Michel and Palm, G{\"u}nther",
  editor    = "Bach, Joscha and Edelkamp, Stefan",
  abstract  = "This paper proposes ``Value-Difference Based Exploration
               combined with Softmax action selection'' (VDBE-Softmax) as an
               adaptive exploration/exploitation policy for temporal-difference
               learning. The advantage of the proposed approach is that
               exploration actions are only selected in situations when the
               knowledge about the environment is uncertain, which is indicated
               by fluctuating values during learning. The method is evaluated
               in experiments having deterministic rewards and a mixture of
               both deterministic and stochastic rewards. The results show that
               a VDBE-Softmax policy can outperform $\epsilon$-greedy, Softmax
               and VDBE policies in combination with onand off-policy learning
               algorithms such as Q-learning and Sarsa. Furthermore, it is also
               shown that VDBE-Softmax is more reliable in case of
               value-function oscillations.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  7006,
  pages     = "335--346",
  series    = "Lecture Notes in Computer Science",
  year      =  2011,
  address   = "Berlin, Heidelberg"
}

@ARTICLE{Schultz1993-zc,
  title    = "Responses of monkey dopamine neurons to reward and conditioned
              stimuli during successive steps of learning a delayed response
              task",
  author   = "Schultz, W and Apicella, P and Ljungberg, T",
  abstract = "The present investigation had two aims: (1) to study responses of
              dopamine neurons to stimuli with attentional and motivational
              significance during several steps of learning a behavioral task,
              and (2) to study the activity of dopamine neurons during the
              performance of cognitive tasks known to be impaired after lesions
              of these neurons. Monkeys that had previously learned a simple
              reaction time task were trained to perform a spatial delayed
              response task via two intermediate tasks. During the learning of
              each new task, a total of 25\% of 76 dopamine neurons showed
              phasic responses to the delivery of primary liquid reward,
              whereas only 9\% of 163 neurons responded to this event once task
              performance was established. This produced an average population
              response during but not after learning of each task. Reward
              responses during learning were significantly more numerous and
              pronounced in area A10, as compared to areas A8 and A9. Dopamine
              neurons also showed phasic responses to the two conditioned
              stimuli. These were the instruction cue, which was the first
              stimulus in each trial and indicated the target of the upcoming
              arm movement (58\% of 76 neurons during and 44\% of 163 neurons
              after learning), and the trigger stimulus, which was a
              conditioned incentive stimulus predicting reward and eliciting a
              saccadic eye movement and an arm reaching movement (38\% of
              neurons during and 40\% after learning). None of the dopamine
              neurons showed sustained activity in the delay between the
              instruction and trigger stimuli that would resemble the activity
              of neurons in dopamine terminal areas, such as the striatum and
              frontal cortex. Thus, dopamine neurons respond phasically to
              alerting external stimuli with behavioral significance whose
              detection is crucial for learning and performing delayed response
              tasks. The lack of sustained activity suggests that dopamine
              neurons do not encode representational processes, such as working
              memory, expectation of external stimuli or reward, or preparation
              of movement. Rather, dopamine neurons are involved with transient
              changes of impulse activity in basic attentional and motivational
              processes underlying learning and cognitive behavior.",
  journal  = "J. Neurosci.",
  volume   =  13,
  number   =  3,
  pages    = "900--913",
  month    =  mar,
  year     =  1993,
  language = "en"
}

@ARTICLE{Hollerman1998-uk,
  title    = "Dopamine neurons report an error in the temporal prediction of
              reward during learning",
  author   = "Hollerman, J R and Schultz, W",
  abstract = "Many behaviors are affected by rewards, undergoing long-term
              changes when rewards are different than predicted but remaining
              unchanged when rewards occur exactly as predicted. The
              discrepancy between reward occurrence and reward prediction is
              termed an 'error in reward prediction'. Dopamine neurons in the
              substantia nigra and the ventral tegmental area are believed to
              be involved in reward-dependent behaviors. Consistent with this
              role, they are activated by rewards, and because they are
              activated more strongly by unpredicted than by predicted rewards
              they may play a role in learning. The present study investigated
              whether monkey dopamine neurons code an error in reward
              prediction during the course of learning. Dopamine neuron
              responses reflected the changes in reward prediction during
              individual learning episodes; dopamine neurons were activated by
              rewards during early trials, when errors were frequent and
              rewards unpredictable, but activation was progressively reduced
              as performance was consolidated and rewards became more
              predictable. These neurons were also activated when rewards
              occurred at unpredicted times and were depressed when rewards
              were omitted at the predicted times. Thus, dopamine neurons code
              errors in the prediction of both the occurrence and the time of
              rewards. In this respect, their responses resemble the teaching
              signals that have been employed in particularly efficient
              computational learning models.",
  journal  = "Nat. Neurosci.",
  volume   =  1,
  number   =  4,
  pages    = "304--309",
  month    =  aug,
  year     =  1998,
  language = "en"
}

@ARTICLE{Howard2018-ky,
  title    = "Identity prediction errors in the human midbrain update
              reward-identity expectations in the orbitofrontal cortex",
  author   = "Howard, James D and Kahnt, Thorsten",
  abstract = "There is general consensus that dopaminergic midbrain neurons
              signal reward prediction errors, computed as the difference
              between expected and received reward value. However, recent work
              in rodents shows that these neurons also respond to errors
              related to inferred value and sensory features, indicating an
              expanded role for dopamine beyond learning cached values. Here we
              utilize a transreinforcer reversal learning task and functional
              magnetic resonance imaging (fMRI) to test whether prediction
              error signals in the human midbrain are evoked when the expected
              identity of an appetitive food odor reward is violated, while
              leaving value matched. We found that midbrain fMRI responses to
              identity and value errors are correlated, suggesting a common
              neural origin for these error signals. Moreover, changes in
              reward-identity expectations, encoded in the orbitofrontal cortex
              (OFC), are directly related to midbrain activity, demonstrating
              that identity-based error signals in the midbrain support the
              formation of outcome identity expectations in OFC.",
  journal  = "Nat. Commun.",
  volume   =  9,
  number   =  1,
  pages    = "1611",
  year     =  2018,
  language = "en"
}

@ARTICLE{Bromberg-Martin2010-tr,
  title    = "Dopamine in motivational control: rewarding, aversive, and
              alerting",
  author   = "Bromberg-Martin, Ethan S and Matsumoto, Masayuki and Hikosaka,
              Okihide",
  abstract = "Midbrain dopamine neurons are well known for their strong
              responses to rewards and their critical role in positive
              motivation. It has become increasingly clear, however, that
              dopamine neurons also transmit signals related to salient but
              nonrewarding experiences such as aversive and alerting events.
              Here we review recent advances in understanding the reward and
              nonreward functions of dopamine. Based on this data, we propose
              that dopamine neurons come in multiple types that are connected
              with distinct brain networks and have distinct roles in
              motivational control. Some dopamine neurons encode motivational
              value, supporting brain networks for seeking, evaluation, and
              value learning. Others encode motivational salience, supporting
              brain networks for orienting, cognition, and general motivation.
              Both types of dopamine neurons are augmented by an alerting
              signal involved in rapid detection of potentially important
              sensory cues. We hypothesize that these dopaminergic pathways for
              value, salience, and alerting cooperate to support adaptive
              behavior.",
  journal  = "Neuron",
  volume   =  68,
  number   =  5,
  pages    = "815--834",
  month    =  dec,
  year     =  2010,
  language = "en"
}

@ARTICLE{Sutton1999-rl,
  title     = "Between {MDPs} and {semi-MDPs}: A framework for temporal
               abstraction in reinforcement learning",
  author    = "Sutton, Richard S and Precup, Doina and Singh, Satinder",
  abstract  = "Learning, planning, and representing knowledge at multiple
               levels of temporal abstraction are key, longstanding challenges
               for AI. In this paper we consider how these challenges can be
               addressed within the mathematical framework of reinforcement
               learning and Markov decision processes (MDPs). We extend the
               usual notion of action in this framework to include
               options---closed-loop policies for taking action over a period
               of time. Examples of options include picking up an object, going
               to lunch, and traveling to a distant city, as well as primitive
               actions such as muscle twitches and joint torques. Overall, we
               show that options enable temporally abstract knowledge and
               action to be included in the reinforcement learning framework in
               a natural and general way. In particular, we show that options
               may be used interchangeably with primitive actions in planning
               methods such as dynamic programming and in learning methods such
               as Q-learning. Formally, a set of options defined over an MDP
               constitutes a semi-Markov decision process (SMDP), and the
               theory of SMDPs provides the foundation for the theory of
               options. However, the most interesting issues concern the
               interplay between the underlying MDP and the SMDP and are thus
               beyond SMDP theory. We present results for three such cases: (1)
               we show that the results of planning with options can be used
               during execution to interrupt options and thereby perform even
               better than planned, (2) we introduce new intra-option methods
               that are able to learn about an option from fragments of its
               execution, and (3) we propose a notion of subgoal that can be
               used to improve the options themselves. All of these results
               have precursors in the existing literature; the contribution of
               this paper is to establish them in a simpler and more general
               setting with fewer changes to the existing reinforcement
               learning framework. In particular, we show that these results
               can be obtained without committing to (or ruling out) any
               particular approach to state abstraction, hierarchy, function
               approximation, or the macro-utility problem.",
  journal   = "Artif. Intell.",
  publisher = "Elsevier",
  volume    =  112,
  number    =  1,
  pages     = "181--211",
  month     =  aug,
  year      =  1999,
  keywords  = "Temporal abstraction; Reinforcement learning; Markov decision
               processes; Options; Macros; Macroactions; Subgoals; Intra-option
               learning; Hierarchical planning; Semi-Markov decision processes"
}

@ARTICLE{Lee2014-sv,
  title    = "Neural computations underlying arbitration between model-based
              and model-free learning",
  author   = "Lee, Sang Wan and Shimojo, Shinsuke and O'Doherty, John P",
  abstract = "There is accumulating neural evidence to support the existence of
              two distinct systems for guiding action selection, a deliberative
              ``model-based'' and a reflexive ``model-free'' system. However,
              little is known about how the brain determines which of these
              systems controls behavior at one moment in time. We provide
              evidence for an arbitration mechanism that allocates the degree
              of control over behavior by model-based and model-free systems as
              a function of the reliability of their respective predictions. We
              show that the inferior lateral prefrontal and frontopolar cortex
              encode both reliability signals and the output of a comparison
              between those signals, implicating these regions in the
              arbitration process. Moreover, connectivity between these regions
              and model-free valuation areas is negatively modulated by the
              degree of model-based control in the arbitrator, suggesting that
              arbitration may work through modulation of the model-free
              valuation system when the arbitrator deems that the model-based
              system should drive behavior.",
  journal  = "Neuron",
  volume   =  81,
  number   =  3,
  pages    = "687--699",
  month    =  feb,
  year     =  2014,
  language = "en"
}

@ARTICLE{Haber2010-ij,
  title     = "The reward circuit: linking primate anatomy and human imaging",
  author    = "Haber, Suzanne N and Knutson, Brian",
  abstract  = "Although cells in many brain regions respond to reward, the
               cortical-basal ganglia circuit is at the heart of the reward
               system. The key structures in this network are the anterior
               cingulate cortex, the orbital prefrontal cortex, the ventral
               striatum, the ventral pallidum, and the midbrain dopamine
               neurons. In addition, other structures, including the dorsal
               prefrontal cortex, amygdala, hippocampus, thalamus, and lateral
               habenular nucleus, and specific brainstem structures such as the
               pedunculopontine nucleus, and the raphe nucleus, are key
               components in regulating the reward circuit. Connectivity
               between these areas forms a complex neural network that mediates
               different aspects of reward processing. Advances in neuroimaging
               techniques allow better spatial and temporal resolution. These
               studies now demonstrate that human functional and structural
               imaging results map increasingly close to primate anatomy.",
  journal   = "Neuropsychopharmacology",
  publisher = "nature.com",
  volume    =  35,
  number    =  1,
  pages     = "4--26",
  month     =  jan,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Frank2011-tv,
  title    = "Computational models of motivated action selection in
              corticostriatal circuits",
  author   = "Frank, Michael J",
  abstract = "Computational models of the basal ganglia have matured and
              received increasing attention over the last decade. This article
              reviews some of the theoretical advances offered by these models,
              focusing on motor and cognitive action selection, learning, and
              the interaction between multiple corticostriatal circuits in
              selection and learning.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  21,
  number   =  3,
  pages    = "381--386",
  month    =  jun,
  year     =  2011,
  language = "en"
}

@ARTICLE{Balleine1998-rv,
  title    = "Goal-directed instrumental action: contingency and incentive
              learning and their cortical substrates",
  author   = "Balleine, B W and Dickinson, A",
  abstract = "Instrumental behaviour is controlled by two systems: a
              stimulus-response habit mechanism and a goal-directed process
              that involves two forms of learning. The first is learning about
              the instrumental contingency between the response and reward,
              whereas the second consists of the acquisition of incentive value
              by the reward. Evidence for contingency learning comes from
              studies of reward devaluation and from demonstrations that
              instrumental performance is sensitive not only the probability of
              contiguous reward but also to the probability of unpaired
              rewards. The process of incentive learning is evident in the
              acquisition of control over performance by primary motivational
              states. Preliminary lesion studies of the rat suggest that the
              prelimbic area of prefrontal cortex plays a role in the
              contingency learning, whereas the incentive learning for food
              rewards involves the insular cortex.",
  journal  = "Neuropharmacology",
  volume   =  37,
  number   = "4-5",
  pages    = "407--419",
  month    =  apr,
  year     =  1998,
  language = "en"
}

@ARTICLE{Evans2003-vl,
  title     = "In two minds: dual-process accounts of reasoning",
  author    = "Evans, Jonathan St B T",
  abstract  = "Researchers in thinking and reasoning have proposed recently
               that there are two distinct cognitive systems underlying
               reasoning. System 1 is old in evolutionary terms and shared with
               other animals: it comprises a set of autonomous subsystems that
               include both innate input modules and domain-specific knowledge
               acquired by a domain-general learning mechanism. System 2 is
               evolutionarily recent and distinctively human: it permits
               abstract reasoning and hypothetical thinking, but is constrained
               by working memory capacity and correlated with measures of
               general intelligence. These theories essentially posit two minds
               in one brain with a range of experimental psychological evidence
               showing that the two systems compete for control of our
               inferences and actions.",
  journal   = "Trends Cogn. Sci.",
  publisher = "Elsevier",
  volume    =  7,
  number    =  10,
  pages     = "454--459",
  month     =  oct,
  year      =  2003,
  language  = "en"
}

@ARTICLE{Langdon2018-jh,
  title    = "Model-based predictions for dopamine",
  author   = "Langdon, Angela J and Sharpe, Melissa J and Schoenbaum, Geoffrey
              and Niv, Yael",
  abstract = "Phasic dopamine responses are thought to encode a
              prediction-error signal consistent with model-free reinforcement
              learning theories. However, a number of recent findings highlight
              the influence of model-based computations on dopamine responses,
              and suggest that dopamine prediction errors reflect more
              dimensions of an expected outcome than scalar reward value. Here,
              we review a selection of these recent results and discuss the
              implications and complications of model-based predictions for
              computational theories of dopamine and learning.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  49,
  pages    = "1--7",
  year     =  2018,
  language = "en"
}

@ARTICLE{Wilson2014-dh,
  title     = "Humans use directed and random exploration to solve the
               explore-exploit dilemma",
  author    = "Wilson, Robert C and Geana, Andra and White, John M and Ludvig,
               Elliot A and Cohen, Jonathan D",
  abstract  = "All adaptive organisms face the fundamental tradeoff between
               pursuing a known reward (exploitation) and sampling lesser-known
               options in search of something better (exploration). Theory
               suggests at least two strategies for solving this dilemma: a
               directed strategy in which choices are explicitly biased toward
               information seeking, and a random strategy in which decision
               noise leads to exploration by chance. In this work we
               investigated the extent to which humans use these two
               strategies. In our ``Horizon task,'' participants made
               explore-exploit decisions in two contexts that differed in the
               number of choices that they would make in the future (the time
               horizon). Participants were allowed to make either a single
               choice in each game (horizon 1), or 6 sequential choices
               (horizon 6), giving them more opportunity to explore. By
               modeling the behavior in these two conditions, we were able to
               measure exploration-related changes in decision making and
               quantify the contributions of the two strategies to behavior. We
               found that participants were more information seeking and had
               higher decision noise with the longer horizon, suggesting that
               humans use both strategies to solve the exploration-exploitation
               dilemma. We thus conclude that both information seeking and
               choice variability can be controlled and put to use in the
               service of exploration.",
  journal   = "J. Exp. Psychol. Gen.",
  publisher = "psycnet.apa.org",
  volume    =  143,
  number    =  6,
  pages     = "2074--2081",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Huys2015-dl,
  title    = "Interplay of approximate planning strategies",
  author   = "Huys, Quentin J M and Lally, N{\'\i}all and Faulkner, Paul and
              Eshel, Neir and Seifritz, Erich and Gershman, Samuel J and Dayan,
              Peter and Roiser, Jonathan P",
  abstract = "Humans routinely formulate plans in domains so complex that even
              the most powerful computers are taxed. To do so, they seem to
              avail themselves of many strategies and heuristics that
              efficiently simplify, approximate, and hierarchically decompose
              hard tasks into simpler subtasks. Theoretical and cognitive
              research has revealed several such strategies; however, little is
              known about their establishment, interaction, and efficiency.
              Here, we use model-based behavioral analysis to provide a
              detailed examination of the performance of human subjects in a
              moderately deep planning task. We find that subjects exploit the
              structure of the domain to establish subgoals in a way that
              achieves a nearly maximal reduction in the cost of computing
              values of choices, but then combine partial searches with greedy
              local steps to solve subtasks, and maladaptively prune the
              decision trees of subtasks in a reflexive manner upon
              encountering salient losses. Subjects come idiosyncratically to
              favor particular sequences of actions to achieve subgoals,
              creating novel complex actions or ``options.''",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  112,
  number   =  10,
  pages    = "3098--3103",
  month    =  mar,
  year     =  2015,
  keywords = "hierarchical reinforcement learning; memoization; planning;
              pruning",
  language = "en"
}

@ARTICLE{Ha2018-up,
  title         = "World Models",
  author        = "Ha, David and Schmidhuber, J{\"u}rgen",
  abstract      = "We explore building generative neural network models of
                   popular reinforcement learning environments. Our world model
                   can be trained quickly in an unsupervised manner to learn a
                   compressed spatial and temporal representation of the
                   environment. By using features extracted from the world
                   model as inputs to an agent, we can train a very compact and
                   simple policy that can solve the required task. We can even
                   train our agent entirely inside of its own hallucinated
                   dream generated by its world model, and transfer this policy
                   back into the actual environment. An interactive version of
                   this paper is available at https://worldmodels.github.io/",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1803.10122"
}

@ARTICLE{Seymour2012-ri,
  title    = "Serotonin selectively modulates reward value in human
              decision-making",
  author   = "Seymour, Ben and Daw, Nathaniel D and Roiser, Jonathan P and
              Dayan, Peter and Dolan, Ray",
  abstract = "Establishing a function for the neuromodulator serotonin in human
              decision-making has proved remarkably difficult because if its
              complex role in reward and punishment processing. In a novel
              choice task where actions led concurrently and independently to
              the stochastic delivery of both money and pain, we studied the
              impact of decreased brain serotonin induced by acute dietary
              tryptophan depletion. Depletion selectively impaired both
              behavioral and neural representations of reward outcome value,
              and hence the effective exchange rate by which rewards and
              punishments were compared. This effect was computationally and
              anatomically distinct from a separate effect on increasing
              outcome-independent choice perseveration. Our results provide
              evidence for a surprising role for serotonin in reward
              processing, while illustrating its complex and multifarious
              effects.",
  journal  = "J. Neurosci.",
  volume   =  32,
  number   =  17,
  pages    = "5833--5842",
  month    =  apr,
  year     =  2012,
  language = "en"
}

@ARTICLE{Lin1992-co,
  title     = "Self-improving reactive agents based on reinforcement learning,
               planning and teaching",
  author    = "Lin, Long-Ji",
  abstract  = "To date, reinforcement learning has mostly been studied solving
               simple learning tasks. Reinforcement learning methods that have
               been studied so far typically converge slowly. The purpose of
               this work is thus two-fold: 1) to investigate the utility of
               reinforcement learning in solving much more complicated learning
               tasks than previously studied, and 2) to investigate methods
               that will speed up reinforcement learning.",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  8,
  number    =  3,
  pages     = "293--321",
  month     =  may,
  year      =  1992
}

@article{Justesen2018-um,
  title         = "Procedural Level Generation Improves Generality of Deep
                   Reinforcement Learning",
  author        = "Justesen, Niels and Torrado, Ruben Rodriguez and Bontrager,
                   Philip and Khalifa, Ahmed and Togelius, Julian and Risi,
                   Sebastian",
  abstract      = "Over the last few years, deep reinforcement learning (RL)
                   has shown impressive results in a variety of domains,
                   learning directly from high-dimensional sensory streams.
                   However, when networks are trained in a fixed environment,
                   such as a single level in a video game, it will usually
                   overfit and fail to generalize to new levels. When RL agents
                   overfit, even slight modifications to the environment can
                   result in poor agent performance. In this paper, we present
                   an approach to prevent overfitting by generating more
                   general agent controllers, through training the agent on a
                   completely new and procedurally generated level each
                   episode. The level generator generate levels whose
                   difficulty slowly increases in response to the observed
                   performance of the agent. Our results show that this
                   approach can learn policies that generalize better to other
                   procedurally generated levels, compared to policies trained
                   on fixed levels.",
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1806.10729",
  journal = "arXiv"
}


@ARTICLE{Kirkpatrick2017-vk,
  title    = "Overcoming catastrophic forgetting in neural networks",
  author   = "Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and
              Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and
              Milan, Kieran and Quan, John and Ramalho, Tiago and
              Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath,
              Claudia and Kumaran, Dharshan and Hadsell, Raia",
  abstract = "The ability to learn tasks in a sequential fashion is crucial to
              the development of artificial intelligence. Until now neural
              networks have not been capable of this and it has been widely
              thought that catastrophic forgetting is an inevitable feature of
              connectionist models. We show that it is possible to overcome
              this limitation and train networks that can maintain expertise on
              tasks that they have not experienced for a long time. Our
              approach remembers old tasks by selectively slowing down learning
              on the weights important for those tasks. We demonstrate our
              approach is scalable and effective by solving a set of
              classification tasks based on a hand-written digit dataset and by
              learning several Atari 2600 games sequentially.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  114,
  number   =  13,
  pages    = "3521--3526",
  year     =  2017,
  keywords = "artificial intelligence; continual learning; deep learning;
              stability plasticity; synaptic consolidation",
  language = "en"
}


@ARTICLE{Gershman2010-oc,
  title     = "Context, learning, and extinction",
  author    = "Gershman, Samuel J and Blei, David M and Niv, Yael",
  abstract  = "A. Redish et al. (2007) proposed a reinforcement learning model
               of context-dependent learning and extinction in conditioning
               experiments, using the idea of ``state classification'' to
               categorize new observations into states. In the current article,
               the authors propose an interpretation of this idea in terms of
               normative statistical inference. They focus on renewal and
               latent inhibition, 2 conditioning paradigms in which contextual
               manipulations have been studied extensively, and show that
               online Bayesian inference within a model that assumes an
               unbounded number of latent causes can characterize a diverse set
               of behavioral results from such manipulations, some of which
               pose problems for the model of Redish et al. Moreover, in both
               paradigms, context dependence is absent in younger animals, or
               if hippocampal lesions are made prior to training. The authors
               suggest an explanation in terms of a restricted capacity to
               infer new causes.",
  journal   = "Psychol. Rev.",
  publisher = "psycnet.apa.org",
  volume    =  117,
  number    =  1,
  pages     = "197--209",
  year      =  2010,
  language  = "en"
}

@ARTICLE{Burgess2002-ny,
  title    = "The human hippocampus and spatial and episodic memory",
  author   = "Burgess, Neil and Maguire, Eleanor A and O'Keefe, John",
  abstract = "Finding one's way around an environment and remembering the
              events that occur within it are crucial cognitive abilities that
              have been linked to the hippocampus and medial temporal lobes.
              Our review of neuropsychological, behavioral, and neuroimaging
              studies of human hippocampal involvement in spatial memory
              concentrates on three important concepts in this field: spatial
              frameworks, dimensionality, and orientation and self-motion. We
              also compare variation in hippocampal structure and function
              across and within species. We discuss how its spatial role
              relates to its accepted role in episodic memory. Five related
              studies use virtual reality to examine these two types of memory
              in ecologically valid situations. While processing of spatial
              scenes involves the parahippocampus, the right hippocampus
              appears particularly involved in memory for locations within an
              environment, with the left hippocampus more involved in
              context-dependent episodic or autobiographical memory.",
  journal  = "Neuron",
  volume   =  35,
  number   =  4,
  pages    = "625--641",
  month    =  aug,
  year     =  2002,
  language = "en"
}

@ARTICLE{Botvinick2009-bl,
  title    = "Hierarchically organized behavior and its neural foundations: a
              reinforcement learning perspective",
  author   = "Botvinick, Matthew and Niv, Yael and Barto, Andrew C",
  abstract = "Research on human and animal behavior has long emphasized its
              hierarchical structure-the divisibility of ongoing behavior into
              discrete tasks, which are comprised of subtask sequences, which
              in turn are built of simple actions. The hierarchical structure
              of behavior has also been of enduring interest within
              neuroscience, where it has been widely considered to reflect
              prefrontal cortical functions. In this paper, we reexamine
              behavioral hierarchy and its neural substrates from the point of
              view of recent developments in computational reinforcement
              learning. Specifically, we consider a set of approaches known
              collectively as hierarchical reinforcement learning, which extend
              the reinforcement learning paradigm by allowing the learning
              agent to aggregate actions into reusable subroutines or skills. A
              close look at the components of hierarchical reinforcement
              learning suggests how they might map onto neural structures, in
              particular regions within the dorsolateral and orbital prefrontal
              cortex. It also suggests specific ways in which hierarchical
              reinforcement learning might provide a complement to existing
              psychological models of hierarchically structured behavior. A
              particularly important question that hierarchical reinforcement
              learning brings to the fore is that of how learning identifies
              new action routines that are likely to provide useful building
              blocks in solving a wide range of future problems. Here and at
              many other points, hierarchical reinforcement learning offers an
              appealing framework for investigating the computational and
              neural underpinnings of hierarchically structured behavior.",
  journal  = "Cognition",
  volume   =  113,
  number   =  3,
  pages    = "262--280",
  year     =  2009,
  language = "en"
}

@BOOK{Kahneman2011-mn,
  title     = "Thinking, fast and slow",
  author    = "Kahneman, Daniel",
  publisher = "Macmillan",
  year      =  2011
}

@ARTICLE{Tobler2003-tz,
  title    = "Coding of predicted reward omission by dopamine neurons in a
              conditioned inhibition paradigm",
  author   = "Tobler, Philippe N and Dickinson, Anthony and Schultz, Wolfram",
  abstract = "Animals learn not only about stimuli that predict reward but also
              about those that signal the omission of an expected reward. We
              used a conditioned inhibition paradigm derived from animal
              learning theory to train a discrimination between a visual
              stimulus that predicted reward (conditioned excitor) and a second
              stimulus that predicted the omission of reward (conditioned
              inhibitor). Performing the discrimination required attention to
              both the conditioned excitor and the inhibitor; however, dopamine
              neurons showed very different responses to the two classes of
              stimuli. Conditioned inhibitors elicited considerable depressions
              in 48 of 69 neurons (median of 35\% below baseline) and minor
              activations in 29 of 69 neurons (69\% above baseline), whereas
              reward-predicting excitors induced pure activations in all 69
              neurons tested (242\% above baseline), thereby demonstrating that
              the neurons discriminated between conditioned stimuli predicting
              reward versus nonreward. The discriminative responses to stimuli
              with differential reward-predicting but common attentional
              functions indicate differential neural coding of reward
              prediction and attention. The neuronal responses appear to
              reflect reward prediction errors, thus suggesting an extension of
              the correspondence between learning theory and activity of single
              dopamine neurons to the prediction of nonreward.",
  journal  = "J. Neurosci.",
  volume   =  23,
  number   =  32,
  pages    = "10402--10410",
  month    =  nov,
  year     =  2003,
  language = "en"
}

@ARTICLE{Schacter_Daniel_L2007-us,
  title     = "The cognitive neuroscience of constructive memory: remembering
               the past and imagining the future",
  author    = "{Schacter Daniel L} and {Addis Donna Rose}",
  journal   = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  publisher = "Royal Society",
  volume    =  362,
  number    =  1481,
  pages     = "773--786",
  month     =  may,
  year      =  2007
}

@ARTICLE{Watkins1992-rn,
  title     = "Q-learning",
  author    = "Watkins, Christopher J C H and Dayan, Peter",
  abstract  = "Q-learning (Watkins, 1989) is a simple way for agents to learn
               how to act optimally in controlled Markovian domains. It amounts
               to an incremental method for dynamic programming which imposes
               limited computational demands. It works by successively
               improving its evaluations of the quality of particular actions
               at particular states.",
  journal   = "Mach. Learn.",
  publisher = "Springer",
  volume    =  8,
  number    =  3,
  pages     = "279--292",
  year      =  1992
}

@BOOK{Clark2015-yq,
  title     = "Surfing Uncertainty: Prediction, Action, and the Embodied Mind",
  author    = "Clark, Andy",
  abstract  = "How is it that thoroughly physical material beings such as
               ourselves can think, dream, feel, create and understand ideas,
               theories and concepts? How does mere matter give rise to all
               these non-material mental states, including consciousness
               itself? An answer to this central question of our existence is
               emerging at the busy intersection of neuroscience, psychology,
               artificial intelligence, and robotics. In this groundbreaking
               work, philosopher and cognitive scientist Andy Clark explores
               exciting new theories from these fields that reveal minds like
               ours to be prediction machines - devices that have evolved to
               anticipate the incoming streams of sensory stimulation before
               they arrive. These predictions then initiate actions that
               structure our worlds and alter the very things we need to engage
               and predict. Clark takes us on a journey in discovering the
               circular causal flows and the self-structuring of the
               environment that define ``the predictive brain.'' What emerges
               is a bold, new, cutting-edge vision that reveals the brain as
               our driving force in the daily surf through the waves of sensory
               stimulation.",
  publisher = "Oxford University Press",
  month     =  oct,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Mnih2016-ai,
  title         = "Asynchronous Methods for Deep Reinforcement Learning",
  author        = "Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and
                   Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and
                   Harley, Tim and Silver, David and Kavukcuoglu, Koray",
  abstract      = "We propose a conceptually simple and lightweight framework
                   for deep reinforcement learning that uses asynchronous
                   gradient descent for optimization of deep neural network
                   controllers. We present asynchronous variants of four
                   standard reinforcement learning algorithms and show that
                   parallel actor-learners have a stabilizing effect on
                   training allowing all four methods to successfully train
                   neural network controllers. The best performing method, an
                   asynchronous variant of actor-critic, surpasses the current
                   state-of-the-art on the Atari domain while training for half
                   the time on a single multi-core CPU instead of a GPU.
                   Furthermore, we show that asynchronous actor-critic succeeds
                   on a wide variety of continuous motor control problems as
                   well as on a new task of navigating random 3D mazes using a
                   visual input.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.01783"
}

@ARTICLE{Doll2012-ou,
  title    = "The ubiquity of model-based reinforcement learning",
  author   = "Doll, Bradley B and Simon, Dylan A and Daw, Nathaniel D",
  abstract = "The reward prediction error (RPE) theory of dopamine (DA)
              function has enjoyed great success in the neuroscience of
              learning and decision-making. This theory is derived from
              model-free reinforcement learning (RL), in which choices are made
              simply on the basis of previously realized rewards. Recently,
              attention has turned to correlates of more flexible, albeit
              computationally complex, model-based methods in the brain. These
              methods are distinguished from model-free learning by their
              evaluation of candidate actions using expected future outcomes
              according to a world model. Puzzlingly, signatures from these
              computations seem to be pervasive in the very same regions
              previously thought to support model-free learning. Here, we
              review recent behavioral and neural evidence about these two
              systems, in attempt to reconcile their enigmatic cohabitation in
              the brain.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  22,
  number   =  6,
  pages    = "1075--1081",
  year     =  2012,
  language = "en"
}

@ARTICLE{Gobet2001-ft,
  title    = "Chunking mechanisms in human learning",
  author   = "Gobet, F and Lane, P C R and Croker, S and Cheng, P C-H and
              Jones, G and Oliver, I and Pine, J M",
  abstract = "Pioneering work in the 1940s and 1950s suggested that the concept
              of 'chunking' might be important in many processes of perception,
              learning and cognition in humans and animals. We summarize here
              the major sources of evidence for chunking mechanisms, and
              consider how such mechanisms have been implemented in
              computational models of the learning process. We distinguish two
              forms of chunking: the first deliberate, under strategic control,
              and goal-oriented; the second automatic, continuous, and linked
              to perceptual processes. Recent work with discrimination-network
              computational models of long- and short-term memory (EPAM/CHREST)
              has produced a diverse range of applications of perceptual
              chunking. We focus on recent successes in verbal learning, expert
              memory, language acquisition and learning multiple
              representations, to illustrate the implementation and use of
              chunking mechanisms within contemporary models of human learning.",
  journal  = "Trends Cogn. Sci.",
  volume   =  5,
  number   =  6,
  pages    = "236--243",
  month    =  jun,
  year     =  2001,
  language = "en"
}

@ARTICLE{Moser2008-km,
  title    = "Place cells, grid cells, and the brain's spatial representation
              system",
  author   = "Moser, Edvard I and Kropff, Emilio and Moser, May-Britt",
  abstract = "More than three decades of research have demonstrated a role for
              hippocampal place cells in representation of the spatial
              environment in the brain. New studies have shown that place cells
              are part of a broader circuit for dynamic representation of
              self-location. A key component of this network is the entorhinal
              grid cells, which, by virtue of their tessellating firing fields,
              may provide the elements of a path integration-based neural map.
              Here we review how place cells and grid cells may form the basis
              for quantitative spatiotemporal representation of places, routes,
              and associated experiences during behavior and in memory. Because
              these cell types have some of the most conspicuous behavioral
              correlates among neurons in nonsensory cortical systems, and
              because their spatial firing structure reflects computations
              internally in the system, studies of entorhinal-hippocampal
              representations may offer considerable insight into general
              principles of cortical network dynamics.",
  journal  = "Annu. Rev. Neurosci.",
  volume   =  31,
  pages    = "69--89",
  year     =  2008,
  language = "en"
}

@ARTICLE{Niv2009-ev,
  title    = "Reinforcement learning in the brain",
  author   = "Niv, Yael",
  abstract = "A wealth of research focuses on the decision-making processes
              that animals and humans employ when selecting actions in the face
              of reward and punishment. Initially such work stemmed from
              psychological investigations of conditioned behavior, and
              explanations of these in terms of computational models.
              Increasingly, analysis at the computational level has drawn on
              ideas from reinforcement learning, which provide a normative
              framework within which decision-making can be analyzed. More
              recently, the fruits of these extensive lines of research have
              made contact with investigations into the neural basis of
              decision making. Converging evidence now links reinforcement
              learning to specific neural substrates, assigning them precise
              computational roles. Specifically, electrophysiological
              recordings in behaving animals and functional imaging of human
              decision-making have revealed in the brain the existence of a key
              reinforcement learning signal, the temporal difference reward
              prediction error. Here, we first introduce the formal
              reinforcement learning framework. We then review the multiple
              lines of evidence linking reinforcement learning to the function
              of dopaminergic neurons in the mammalian midbrain and to more
              recent data from human imaging experiments. We further extend the
              discussion to aspects of learning not associated with phasic
              dopamine signals, such as learning of goal-directed responding
              that may not be dopamine-dependent, and learning about the vigor
              (or rate) with which actions should be performed that has been
              linked to tonic aspects of dopaminergic signaling. We end with a
              brief discussion of some of the limitations of the reinforcement
              learning framework, highlighting questions for future research.",
  journal  = "J. Math. Psychol.",
  volume   =  53,
  number   =  3,
  pages    = "139--154",
  year     =  2009
}

@INPROCEEDINGS{Tokic2010-pr,
  title      = "Adaptive $\epsilon$-greedy Exploration in Reinforcement
                Learning Based on Value Differences",
  booktitle  = "{KI} 2010: Advances in Artificial Intelligence",
  author     = "Tokic, Michel",
  editor     = "{Dillmann R., Beyerer J., Hanebeck U.D., Schultz T.}",
  abstract   = "This paper presents ``Value-Difference Based Exploration''
                (VDBE), a method for balancing the exploration/exploitation
                dilemma inherent to reinforcement learning. The proposed method
                adapts the exploration parameter of $\epsilon$-greedy in
                dependence of the temporal-difference error observed from
                value-function backups, which is considered as a measure of the
                agent's uncertainty about the environment. VDBE is evaluated on
                a multi-armed bandit task, which allows for insight into the
                behavior of the method. Preliminary results indicate that VDBE
                seems to be more parameter robust than commonly used ad hoc
                approaches such as $\epsilon$-greedy or softmax.",
  publisher  = "Springer, Berlin, Heidelberg",
  year       =  2010,
  conference = "KI 2010: Advances in Artificial Intelligence"
}


@ARTICLE{Collins2013-cv,
  title    = "Cognitive control over learning: creating, clustering, and
              generalizing task-set structure",
  author   = "Collins, Anne G E and Frank, Michael J",
  abstract = "Learning and executive functions such as task-switching share
              common neural substrates, notably prefrontal cortex and basal
              ganglia. Understanding how they interact requires studying how
              cognitive control facilitates learning but also how learning
              provides the (potentially hidden) structure, such as abstract
              rules or task-sets, needed for cognitive control. We investigate
              this question from 3 complementary angles. First, we develop a
              new context-task-set (C-TS) model, inspired by nonparametric
              Bayesian methods, specifying how the learner might infer hidden
              structure (hierarchical rules) and decide to reuse or create new
              structure in novel situations. Second, we develop a
              neurobiologically explicit network model to assess mechanisms of
              such structured learning in hierarchical frontal cortex and basal
              ganglia circuits. We systematically explore the link between
              these modeling levels across task demands. We find that the
              network provides an approximate implementation of high-level C-TS
              computations, with specific neural mechanisms modulating distinct
              C-TS parameters. Third, this synergism yields predictions about
              the nature of human optimal and suboptimal choices and response
              times during learning and task-switching. In particular, the
              models suggest that participants spontaneously build task-set
              structure into a learning problem when not cued to do so, which
              predicts positive and negative transfer in subsequent
              generalization tests. We provide experimental evidence for these
              predictions and show that C-TS provides a good quantitative fit
              to human sequences of choices. These findings implicate a strong
              tendency to interactively engage cognitive control and learning,
              resulting in structured abstract representations that afford
              generalization opportunities and, thus, potentially long-term
              rather than short-term optimality.",
  journal  = "Psychol. Rev.",
  volume   =  120,
  number   =  1,
  pages    = "190--229",
  year     =  2013,
  language = "en"
}


@ARTICLE{Collins2019-of,
  title    = "Reinforcement learning: bringing together computation and
              cognition",
  author   = "Collins, Anne Gabrielle Eva",
  abstract = "A key aspect of human intelligence is our ability to learn very
              quickly. This ability is still lacking in artificial
              intelligence. This article will highlight recent research showing
              how bringing together the fields of artificial intelligence and
              cognitive science may benefit both. Ideas from artificial
              intelligence have provided helpful formal theories to account for
              aspects of human learning. In return, ideas from cognitive
              science and neuroscience can also inform artificial intelligence
              research with directions to make algorithms more human-like. For
              example, recent work shows that human learning can only be
              understood in the context of multiple separate, interacting
              memory systems, rather than as a single, complex learner. This
              insight is starting to show promise in improving artificial
              agents' learning efficiency.",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  29,
  pages    = "63--68",
  year     =  2019
}


@ARTICLE{Collins2018-ga,
  title    = "Within- and across-trial dynamics of human {EEG} reveal
              cooperative interplay between reinforcement learning and working
              memory",
  author   = "Collins, Anne G E and Frank, Michael J",
  abstract = "Learning from rewards and punishments is essential to survival
              and facilitates flexible human behavior. It is widely appreciated
              that multiple cognitive and reinforcement learning systems
              contribute to decision-making, but the nature of their
              interactions is elusive. Here, we leverage methods for extracting
              trial-by-trial indices of reinforcement learning (RL) and working
              memory (WM) in human electro-encephalography to reveal
              single-trial computations beyond that afforded by behavior alone.
              Neural dynamics confirmed that increases in neural expectation
              were predictive of reduced neural surprise in the following
              feedback period, supporting central tenets of RL models. Within-
              and cross-trial dynamics revealed a cooperative interplay between
              systems for learning, in which WM contributes expectations to
              guide RL, despite competition between systems during choice.
              Together, these results provide a deeper understanding of how
              multiple neural systems interact for learning and decision-making
              and facilitate analysis of their disruption in clinical
              populations.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  115,
  number   =  10,
  pages    = "2502--2507",
  year     =  2018,
  keywords = "EEG; computational model; dynamics; reinforcement learning;
              working memory",
  language = "en"
}


@ARTICLE{Momennejad2020-lk,
  title    = "Learning Structures: Predictive Representations, Replay, and
              Generalization",
  author   = "Momennejad, Ida",
  abstract = "Memory and planning rely on learning the structure of
              relationships among experiences. Compact representations of these
              structures guide flexible behavior in humans and animals. A
              century after `latent learning' experiments summarized by Tolman,
              the larger puzzle of cognitive maps remains elusive: how does the
              brain learn and generalize relational structures? This review
              focuses on a reinforcement learning (RL) approach to learning
              compact representations of the structure of states. We review
              evidence showing that capturing structures as predictive
              representations updated via replay offers a neurally plausible
              account of human behavior and the neural representations of
              predictive cognitive maps. We highlight multi-scale successor
              representations, prioritized replay, and policy-dependence. These
              advances call for new directions in studying the entanglement of
              learning and memory with prediction and planning.",
  journal  = "Current Opinion in Behavioral Sciences",
  volume   =  32,
  pages    = "155--166",
  year     =  2020
}


@article{Eckstein2020-yi,
  title   = "Computational Evidence for {Hierarchically-Structured}
             Reinforcement Learning in Humans",
  author  = "Eckstein, Maria K and Collins, Anne G E",
  journal = "bioRxiv",
  year    =  2020
}

@article{OpenAI2019-xb,
    title={Dota 2 with Large Scale Deep Reinforcement Learning},
    author={OpenAI and : and Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemysław Dębiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Chris Hesse and Rafal Józefowicz and Scott Gray and Catherine Olsson and Jakub Pachocki and Michael Petrov and Henrique Pondé de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
    year={2019},
    eprint={1912.06680},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    journal="arXiv"
}

@article{gershman2014statistical,
  title={Statistical computations underlying the dynamics of memory updating},
  author={Gershman, Samuel J and Radulescu, Angela and Norman, Kenneth A and Niv, Yael},
  journal={PLoS computational biology},
  volume={10},
  number={11},
  year={2014},
  publisher={Public Library of Science}
}
